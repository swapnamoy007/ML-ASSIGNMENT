# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KZg08wreUBXtXvaK9Pbj0yAcQRliWoyQ
"""

#"swapnamoy dutta"

#"Bredesen center for interdisciplinary research and graduate education (ESE)"

# PART 1
#I have explored these 4 articles (Due to having close correlation with my current research field):-
#Explore these 3 papers
#1. A comparison of artificial intelligence models for predicting phosphate removal efficiency from wastewater using the electrocoagulation process
#2. Operational parameter prediction of electrocoagulation system in a rural decentralized water treatment plant by interpretable machine learning model
#3. Modelling and optimisation of electrocoagulation/flocculation recovery of effluent from land-based aquaculture by artificial intelligence (AI) approaches
#4. Parametric optimization and modeling of continuous electrocoagulation process for the removal of fluoride: Response surface methodology and machine learning approach

#A Comparison of Artificial Intelligence Models for Predicting Phosphate Removal Efficiency from Wastewater Using the Electrocoagulation Process
#Summary: This paper compares various artificial intelligence (AI) models in predicting the efficiency of phosphate removal from wastewater using the electrocoagulation process. The AI models studied include Artificial Neural Networks (ANN), Support Vector Machines (SVM), and Random Forests (RF). The study aims to determine which model provides the most accurate predictions based on operational parameters such as pH, current density, and electrode material.

#Motivation: Phosphate removal is critical for preventing eutrophication in water bodies. Traditional modeling approaches can be complex and time-consuming, making AI models a viable alternative for predicting treatment outcomes with greater accuracy and less computational effort.

#Data and Findings:
#Performance Metrics: The paper reports on various performance metrics such as Mean Squared Error (MSE), R-squared (R²), and Root Mean Squared Error (RMSE) for each model.
#Results:ANN outperformed the other models with an R² value of 0.95, indicating a strong correlation between predicted and actual phosphate removal efficiency.
SVM and RF also provided good predictions, but with slightly lower R² values of 0.92 and 0.90, respectively.
Table: Model Performance Comparison
Model	R² Value	MSE	RMSE
ANN	0.95	0.003	0.055
SVM	0.92	0.004	0.063
RF	0.90	0.005	0.071
Knowledge Graph: AI Model Selection for Phosphate Removal Efficiency
Nodes: Phosphate Removal Efficiency, ANN, SVM, RF
Edges: Predicts (Efficiency), Compares (Models), Outputs (Performance Metrics)
Key Methodologies:
Data Collection: The study used experimental data from various electrocoagulation trials, where variables like current density, pH, and electrode material were systematically varied.
Model Training: Each AI model was trained on a subset of the data, and performance was assessed using a test set. The models were evaluated based on metrics like R², MSE, and RMSE.
Results:
ANN Performance: The ANN model achieved an R² of 0.95, demonstrating high predictive accuracy. The model was particularly effective at capturing nonlinear relationships between the input parameters and phosphate removal efficiency.
SVM and RF Performance: SVM and RF models also performed well, with R² values of 0.92 and 0.90, respectively. While these models were slightly less accurate than ANN, they were still effective for predictive purposes.

#Operational Parameter Prediction of Electrocoagulation System in a Rural Decentralized Water Treatment Plant by Interpretable Machine Learning Model
#Summary:This study focuses on predicting operational parameters for an electrocoagulation system in a rural decentralized water treatment plant using interpretable machine learning models. The emphasis is on making the models not only accurate but also understandable to operators who may not have technical expertise in AI.

#Motivation:Rural water treatment systems often lack the resources and expertise found in urban settings. Thus, developing machine learning models that are interpretable and easy to implement is crucial for ensuring these systems operate efficiently and sustainably.

#Data and Findings: Models Used: The study primarily uses Decision Trees and Linear Regression, chosen for their interpretability.
Results:
The Decision Tree model provided clear insights into the influence of each operational parameter, such as current density and pH, on the overall system performance.
Linear Regression models, while slightly less accurate, offered a straightforward understanding of parameter relationships, making it easier for non-experts to manage the system.
Table: Model Insights
Model	Interpretability	R² Value	Key Insights
Decision Tree	- High -	0.89;	Current density is the most influential parameter.
Linear Regression- 	Moderate	- 0.85	Direct correlation between pH and efficiency.
Knowledge Graph: Operational Parameter Prediction
Nodes: Electrocoagulation, Decision Tree, Linear Regression, Current Density, pH, Efficiency
Edges: Predicts (Operational Parameters), Explains (Model Interpretability), Correlates (Parameters)
Key Methodologies:
Model Selection: The study selected Decision Trees and Linear Regression models for their interpretability. These models provide insights into which parameters (e.g., current density, electrode spacing) have the most significant impact on treatment efficiency.
Model Validation: Cross-validation techniques were used to ensure the models generalize well to new data.
Results:
Decision Tree Insights: The Decision Tree model identified current density as the most influential parameter, followed by pH and electrode material. This model achieved an R² of 0.89.
Linear Regression Insights: While slightly less accurate (R² of 0.85), the Linear Regression model provided a clear linear relationship between input parameters and treatment efficiency, making it easier to implement in practice

# Modelling and Optimisation of Electrocoagulation/Flocculation Recovery of Effluent from Land-Based Aquaculture by Artificial Intelligence (AI) Approaches
#Summary: This paper investigates the use of AI models to optimize the electrocoagulation/flocculation process for recovering effluent from land-based aquaculture systems. The study compares models like ANN, Genetic Algorithms (GA), and Hybrid Models to determine the most effective approach for maximizing effluent recovery while minimizing energy consumption.

#Motivation: Effluent management in aquaculture is critical for maintaining water quality and preventing environmental degradation. AI models offer a way to optimize this process by balancing recovery efficiency with energy use, a key concern in sustainable aquaculture practices.

Data and Findings:
Performance Comparison: The study compares the efficiency of effluent recovery and energy consumption across different models.
Results:
The Hybrid Model combining ANN and GA achieved the highest recovery efficiency (93%) with optimized energy consumption.
ANN alone also performed well but required slightly more energy compared to the hybrid model.
GA was effective in optimizing individual parameters but lacked the predictive accuracy of the other models when used alone.
Table: Effluent Recovery and Energy Consumption
Model	Recovery Efficiency (%)	Energy Consumption (kWh)	Optimization Approach
ANN	                   90	           0.8	                  Prediction
GA	                   85	           0.75	             Parameter Optimization
Hybrid	               93	           0.7	              Combined Optimization
Knowledge Graph: AI in Effluent Recovery Optimization
Nodes: Effluent Recovery, Energy Consumption, ANN, GA, Hybrid Model
Edges: Optimizes (Recovery Efficiency), Minimizes (Energy Consumption), Combines (ANN and GA)
Hybrid Modeling: The study combines ANN and GA to optimize both the prediction of effluent recovery efficiency and the reduction of energy consumption.
Model Optimization: GA is used to fine-tune the ANN parameters, resulting in a hybrid model that improves both prediction accuracy and process efficiency.
Results:
Hybrid Model Performance: The ANN-GA hybrid model achieved a recovery efficiency of 93% with optimized energy consumption. This model outperformed both ANN and GA when used separately.
Energy Consumption: The hybrid model was also the most energy-efficient, consuming 0.7 kWh, compared to 0.8 kWh for ANN and 0.75 kWh for GA alone.

#Parametric Optimization and Modeling of Continuous Electrocoagulation Process for the Removal of Fluoride: Response Surface Methodology and Machine Learning Approach
#Overview: This paper focuses on optimizing the continuous electrocoagulation process for fluoride removal using a combination of Response Surface Methodology (RSM) and machine learning models. The study aims to identify the optimal operational parameters that maximize fluoride removal while minimizing energy consumption.

#Key Methodologies: RSM Approach: RSM is used to design the experiments and identify the relationships between process variables (e.g., current density, electrode spacing) and fluoride removal efficiency.
Machine Learning Models: The study employs models like ANN and SVM to predict and optimize the fluoride removal process based on the data generated by RSM.
Results:
Optimal Conditions: The study identified that a current density of 25 mA/cm², electrode spacing of 2 cm, and a treatment time of 20 minutes yielded the highest fluoride removal efficiency (approximately 95%).
Model Comparison: ANN outperformed SVM in terms of predictive accuracy, with an R² of 0.96 compared to SVM’s R² of 0.93.
Table: Optimal Parameters and Fluoride Removal Efficiency
Parameter	Optimal Value	Predicted Efficiency (%)	Experimental Efficiency (%)
Current Density	25 mA/cm²	95	94.5
Electrode Spacing	2 cm	95	94.8
Treatment Time	20 minutes	95	95
Analysis:
This study successfully integrates RSM and machine learning to optimize the electrocoagulation process for fluoride removal. The high predictive accuracy of the ANN model, combined with the experimental validation, demonstrates the potential of these approaches for optimizing complex water treatment processes.

# Part 2

#Chapter 3

"""**Excercise** 34-57"""

import math

numbers = [5, 7, 11]

result = sum([math.factorial(n) for n in numbers])

print(result)

import math
import pandas

def compute(numbers):
    return([math.factorial(n) for n in numbers])
    from my_module import compute
compute([5, 7, 11])

""" This script computes the sum of the factorial of a list of numbers"""

!pip install my_module

!pip install math

import my_module

help(my_module)

my_module.__doc__

import datetime
print(datetime.date.today())

l = [4, 2, 7, 3]

maximum = 0

for number in l:
    if number > maximum:
        maximum = number

print(maximum)

l = [5, 8, 1, 3, 2]

still_swapping = True

while still_swapping:
    still_swapping = False
    for i in range(len(l) - 1):
        if l[i] > l[i+1]:
            l[i], l[i+1] = l[i+1], l[i]
            still_swapping = True

l

l = [5, 8, 1, 3, 2]

search_for = 8

result = -1

for i in range(len(l)):
    if search_for == l[i]:
        result = i
        break

print(result)

l = [2, 3, 5, 8, 11, 12, 18]

search_for = 11

slice_start = 0

slice_end = len(l) - 1

found = False

# Search only till both start and end of sublist meet and found
while slice_start <= slice_end and not found:

    # Find the center of sublist
    location = (slice_start + slice_end) // 2

    # If a match then element is found
    if l[location] == search_for:
        found = True

    # If match not found change start or end of subset array
    else:
        if search_for < l[location]:
            slice_end = location - 1
        else:
            slice_start = location + 1

print(found)
if found:
    print(location)

def get_second_element(mylist):
    if len(mylist) > 1:
        return mylist[1]
    else:
        return 'List was too small'

get_second_element([1, 2, 3])

get_second_element([1])

def list_product(my_list):
    result = 1
    for number in my_list:
         result = result * number
    return result

print(list_product([2, 3]))
print(list_product([2, 10, 15]))

def list_product(my_list):
    result = 1
    for number in my_list:
         result = result * number
    return result

print(list_product([2, 3]))
print(list_product([2, 10, 15]))

def add_suffix(suffix='.com'):
    return 'google' + suffix

add_suffix()

add_suffix('.co.uk')

def convert_usd_to_aud(amount, rate=0.75):
    return amount / rate

convert_usd_to_aud(100)

convert_usd_to_aud(100, rate=0.78)

def sum_of_numbers(*args):
    total = 0
    for number in args:
        total += number
    return total

print(sum_of_numbers(1,3,2,5,4))

def person_details(**kwargs):
    print("Personal Details")
    for key, value in kwargs.items():
        print("{} : {}".format(key,value))

person_details(firstName = "Jason", lastName = "Scott", country = "US")

person_details(name = "Ratan", country = "India", age = 23)

input_dict = {'name' : 'Vikas Singh' , 'gender' : 'Male'}
person_details(**input_dict)

def sum_first_n(n):
    result = 0
    for i in range(n):
        result += i + 1
    return result

sum_first_n(100)

def is_prime(x):
    for i in range(2, x):
        if (x % i) == 0:
            return False
    return True

is_prime(7)

is_prime(1000)

def countdown(n):
    if n == 0:
        print('liftoff!')
    else:
        print(n)
        return countdown(n - 1)

countdown(3)

def factorial_iterative(n):
    result = 1
    for i in range(n):
        result *= i + 1
    return result

factorial_iterative(5)

def factorial_recursive(n):
    if n == 1 or n==0:
        return 1
    else:
        return n * factorial_recursive(n - 1)

factorial_recursive(5)

stored_results = {}

def sum_to_n(n):
    result = 0

stored_results = {}

def sum_to_n(n):
    result = 0
    for i in reversed(range(n)):
        result += i + 1
    stored_results[n] = result
    return result

stored_results = {}

def sum_to_n(n):
    result = 0
    for i in reversed(range(n)):
        if i + 1 in stored_results:
            print('Stopping sum at %s because we have previously computed it' % str(i + 1))
            result += stored_results[i + 1]
            break
        else:
            result += i + 1
    stored_results[n] = result
    return result

sum_to_n(5)

sum_to_n(6)

import time

stored_results = {}
def sum_to_n(n):
    start_time = time.perf_counter()
    result = 0
    for i in reversed(range(n)):
        if i + 1 in stored_results:
            print('Stopping sum at %s because we have previously computed it' % str(i + 1))
            result += stored_results[i + 1]
            break
        else:
            result += i + 1
    stored_results[n] = result
    print(time.perf_counter() - start_time, "seconds")
    return result

sum_to_n(1000000)

sum_to_n(1000000)

def compute_usd_total(amount_in_aud=0, amount_in_gbp=0):
    total = 0
    total += amount_in_aud * 0.78
    total += amount_in_gbp * 1.29
    return total

compute_usd_total(amount_in_gbp=10)

def convert_currency(amount, rate, margin=0):
     return amount * rate * (1 + margin)

def compute_usd_total(amount_in_aud=0, amount_in_gbp=0):
    total = 0
    total += convert_currency(amount_in_aud, 0.78)
    total += convert_currency(amount_in_gbp, 1.29)
    return total

compute_usd_total(amount_in_gbp=10)

def compute_usd_total(amount_in_aud=0, amount_in_gbp=0):
    total = 0
    total += convert_currency(amount_in_aud, 0.78)
    total += convert_currency(amount_in_gbp, 1.29, 0.01)
    return total

compute_usd_total(amount_in_gbp=10)

first_item = lambda my_list: my_list[0]

first_item(['cat', 'dog', 'mouse'])

nums = [-3, -5, 1, 4]

list(map(lambda x: 1 / (1 + math.exp(-x)), nums))

nums = list(range(1000))

filtered = filter(lambda x: x % 3 == 0 or x % 7 == 0, nums)

sum(filtered)

import datetime

# Using the datetime library, we can get the current datetime stamp,
# and then call the time() function in order to retrieve the time
time = datetime.datetime.now().time()

# If the script is being executed, this if statement will be true,
# and therefore the time will be printed
if __name__ == '__main__':
    print(time)



!pip uninstall customer

!pip install customer

def format_customer(first, last, location=None):
    full_name = '{} {}'.format(first, last)
    if location:
        return '{} ({})'.format(full_name, location)
    else:
        return full_name

from customer import format_customer

format_customer('John', 'Smith', location='California')

format_customer('Mareike', 'Schmidt')

from fibonacci import fibonacci_iterative

fibonacci_iterative(3)

fibonacci_iterative(10)

!pip install fibonacci

!pip install fibonacci_recursive

def fibonacci_iterative(n):
    previous = 0
    current = 1
    for i in range(n - 1):
        current_old = current
        current = previous + current
        previous = current_old
    return current
def fibonacci_recursive(n):
    if n == 0 or n == 1:
        return n
    else:
        return fibonacci_recursive(n - 2) + fibonacci_recursive(n - 1)

fibonacci_recursive(3)

fibonacci_recursive(10)

def fibonacci_iterative(n):
    previous = 0
    current = 1
    for i in range(n - 1):
        current_old = current
        current = previous + current
        previous = current_old
    return current
stored = {0: 0, 1: 1}  # We set the first 2 terms of the Fibonacci sequence here.
def fibonacci_recursive(n):
    if n == 0 or n == 1:
        return n
    else:
        return fibonacci_recursive(n - 2) + fibonacci_recursive(n - 1)
def fibonacci_dynamic(n):
    if n in stored:
        return stored[n]
    else:
        stored[n] = fibonacci_dynamic(n - 2) + fibonacci_dynamic(n - 1)
        return stored[n]

fibonacci_dynamic(3)

fibonacci_dynamic(100)

"""Chapter 4

Example 58-69
"""

with open('pg37431.txt', 'rb') as f:
    text = f.read()
print(text)

with open("pg37431.txt") as f:
    print(f.read(5))

with open("pg37431.txt") as f:
    print(f.readline())

f = open('log.txt', 'w')

from datetime import datetime
import time

for i in range(0,10):
    print(datetime.now().strftime('%Y%m%d_%H:%M:%S - '),i)
    f.write(datetime.now().strftime('%Y%m%d_%H:%M:%S - '));
    time.sleep(1)
    f.write(str(i));
    f.write("\n")
f.close()

def avg(marks):
    assert len(marks) != 0
    return round(sum(marks)/len(marks), 2)

sem1_marks = [62, 65, 75]
print("Average marks for semester 1:",avg(sem1_marks))

ranks = []
print("Average of marks for semester 1:",avg(ranks))

import matplotlib.pyplot as plt

temperature = [14.2, 16.4, 11.9, 12.5, 18.9, 22.1, 19.4, 23.1, 25.4, 18.1, 22.6, 17.2]
sales = [215.20, 325.00, 185.20, 330.20, 418.60, 520.25, 412.20, 614.60, 544.80, 421.40, 445.50, 408.10]

plt.scatter(temperature, sales, color='red')
plt.show()

plt.title('Ice-cream sales versus Temperature')
plt.xlabel('Temperature')
plt.ylabel('Sales')
plt.scatter(temperature, sales, color='red')
plt.show()

stock_price = [190.64, 190.09, 192.25, 191.79, 194.45, 196.45, 196.45, 196.42, 200.32, 200.32, 200.85, 199.2, 199.2, 199.2, 199.46, 201.46, 197.54, 201.12, 203.12, 203.12, 203.12, 202.83, 202.83, 203.36, 206.83, 204.9, 204.9, 204.9, 204.4, 204.06]

import matplotlib.pyplot as plt
plt.plot(stock_price)
plt.title('Opening Stock Prices')
plt.xlabel('Days')
plt.ylabel('$ USD')
plt.show()

t = list(range(1, 31))

plt.plot(t, stock_price, marker='.', color='red')
plt.xticks([1, 8, 15, 22, 28])

stock_price = [190.64, 190.09, 192.25, 191.79, 194.45, 196.45, 196.45, 196.42, 200.32, 200.32, 200.85, 199.2, 199.2, 199.2, 199.46, 201.46, 197.54, 201.12, 203.12, 203.12, 203.12, 202.83, 202.83, 203.36, 206.83, 204.9, 204.9, 204.9, 204.4, 204.06]

t = list(range(1, 31))

plt.title('Opening Stock Prices')
plt.xlabel('Days')
plt.ylabel('$ USD')

plt.plot(t, stock_price, marker='.', color='red')
plt.xticks([1, 8, 15, 22, 28])

plt.show()

grades = ['A', 'B', 'C', 'D', 'E', 'F']
students_count = [20, 30, 10, 5, 8, 2]

import matplotlib.pyplot as plt
plt.bar(grades, students_count, color=['green', 'gray', 'gray', 'gray', 'gray', 'red'])

plt.title('Grades Bar Plot for Biology Class')
plt.xlabel('Grade')
plt.ylabel('Num Students')
plt.bar(grades, students_count, color=['green', 'gray', 'gray', 'gray', 'gray', 'red'])
plt.show()

plt.barh(grades, students_count, color=['green', 'gray', 'gray', 'gray', 'gray', 'red'])

# Plotting
labels = ['Monica', 'Adrian', 'Jared']
num = [230, 100, 98] # Note that this does not need to be percentages

import matplotlib.pyplot as plt
plt.pie(num, labels=labels, autopct='%1.1f%%', colors=['lightblue', 'lightgreen', 'yellow'])

plt.title('Voting Results: Club President', fontdict={'fontsize': 20})
plt.pie(num, labels=labels, autopct='%1.1f%%', colors=['lightblue', 'lightgreen', 'yellow'])
plt.show()

def heatmap(data, row_labels, col_labels, ax=None,
            cbar_kw={}, cbarlabel="", **kwargs):
    if not ax:
        ax = plt.gca()

    im = ax.imshow(data, **kwargs)
    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)
    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va="bottom")
    ax.set_xticks(np.arange(data.shape[1]))
    ax.set_yticks(np.arange(data.shape[0]))
    ax.set_xticklabels(col_labels)
    ax.set_yticklabels(row_labels)
    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)
    plt.setp(ax.get_xticklabels(), rotation=-30, ha="right", rotation_mode="anchor")
    for edge, spine in ax.spines.items():
        spine.set_visible(False)
    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)
    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)
    ax.grid(which="minor", color="w", linestyle='-', linewidth=3)
    ax.tick_params(which="minor", bottom=False, left=False)
    return im, cbar

import numpy as np
import matplotlib.pyplot as plt
data = np.array([
    [30, 20, 10,],
    [10, 40, 15],
    [12, 10, 20]
])

im, cbar = heatmap(data, ['Class-1', 'Class-2', 'Class-3'], ['A', 'B', 'C'], cmap='YlGn', cbarlabel='Number of Students')

def annotate_heatmap(im, data=None, valfmt="{x:.2f}",
                     textcolors=["black", "white"],
                     threshold=None, **textkw):
    import matplotlib
    if not isinstance(data, (list, np.ndarray)):
        data = im.get_array()
    if threshold is not None:
        threshold = im.norm(threshold)
    else:
        threshold = im.norm(data.max())/2.
    kw = dict(horizontalalignment="center",
              verticalalignment="center")
    kw.update(textkw)
    if isinstance(valfmt, str):
        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)
    texts = []
    for i in range(data.shape[0]):
        for j in range(data.shape[1]):
            kw.update(color=textcolors[im.norm(data[i, j]) > threshold])
            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)
            texts.append(text)

    return texts

im, cbar = heatmap(data, ['Class-1', 'Class-2', 'Class-3'], ['A', 'B', 'C'], cmap='YlGn', cbarlabel='Number of Students')

texts = annotate_heatmap(im, valfmt="{x}")

import seaborn as sns
data = [90, 80, 50, 42, 89, 78, 34, 70, 67, 73, 74, 80, 60, 90, 90]
sns.distplot(data)

import matplotlib.pyplot as plt
plt.title('Density Plot')
plt.xlabel('Score')
plt.ylabel('Density')
sns.distplot(data)
plt.show()

weight=[85.08,79.25,85.38,82.64,80.51,77.48,79.25,78.75,77.21,73.11,82.03,82.54,74.62,79.82,79.78,77.94,83.43,73.71,80.23,78.27,78.25,80.00,76.21,86.65,78.22,78.51,79.60,83.88,77.68,78.92,79.06,85.30,82.41,79.70,80.16,81.11,79.58,77.42,75.82,74.09,78.31,83.17,75.20,76.14]

!pip install matplotlib

!pip install numpy

!pip install sns

import seaborn as sns
sns.kdeplot(list(range(1,45)),weight, kind='kde', cmap="Reds", )

plt.title('Weight Dataset - Contour Plot')
plt.ylabel('height (cm)')
plt.xlabel('weight (cm)')
sns.kdeplot(list(range(1,45)),weight, kind='kde', cmap="Reds", )

from mpl_toolkits.mplot3d import Axes3D
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
X = np.linspace(0, 10, 50)
Y = np.linspace(0, 10, 50)
X, Y = np.meshgrid(X, Y)
Z = (np.sin(X))

# Setup axis
fig = plt.figure(figsize=(7,5))
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X, Y, Z)

# Add title and axes labels
ax.set_title("Demo of 3D Plot", size=13)
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')

import numpy as np
import matplotlib.pyplot as plt
def heatmap(data, row_labels, col_labels, ax=None,
            cbar_kw={}, cbarlabel="", **kwargs):
    if not ax:
        ax = plt.gca()

    im = ax.imshow(data, **kwargs)
    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)
    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va="bottom")
    ax.set_xticks(np.arange(data.shape[1]))
    ax.set_yticks(np.arange(data.shape[0]))
    ax.set_xticklabels(col_labels)
    ax.set_yticklabels(row_labels)
    ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)
    plt.setp(ax.get_xticklabels(), rotation=-30, ha="right", rotation_mode="anchor")
    for edge, spine in ax.spines.items():
        spine.set_visible(False)
    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)
    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)
    ax.grid(which="minor", color="w", linestyle='-', linewidth=3)
    ax.tick_params(which="minor", bottom=False, left=False)
    return im, cbar

data = np.array([
    [30, 20, 10,],
    [10, 40, 15],
    [12, 10, 20]
])

im, cbar = heatmap(data, ['Class-1', 'Class-2', 'Class-3'], ['A', 'B', 'C'], cmap='YlGn', cbarlabel='Number of Students')

def annotate_heatmap(im, data=None, valfmt="{x:.2f}",
                     textcolors=["black", "white"],
                     threshold=None, **textkw):
    import matplotlib
    if not isinstance(data, (list, np.ndarray)):
        data = im.get_array()
    if threshold is not None:
        threshold = im.norm(threshold)
    else:
        threshold = im.norm(data.max())/2.
    kw = dict(horizontalalignment="center",
              verticalalignment="center")
    kw.update(textkw)
    if isinstance(valfmt, str):
        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)
    texts = []
    for i in range(data.shape[0]):
        for j in range(data.shape[1]):
            kw.update(color=textcolors[im.norm(data[i, j]) > threshold])
            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)
            texts.append(text)

    return texts

im, cbar = heatmap(data, ['Class-1', 'Class-2', 'Class-3'], ['A', 'B', 'C'], cmap='YlGn', cbarlabel='Number of Students')

texts = annotate_heatmap(im, valfmt="{x}")

#Activity 13

import csv

lines = []

with open('titanic_train.csv') as csv_file:
    csv_reader = csv.reader(csv_file, delimiter=',')
    for line in csv_reader:
        lines.append(line)

data = lines[1:]
passengers = []
headers = lines[0]

for d in data:
    p = {}
    for i in range(0,len(headers)):
        key = headers[i]
        value = d[i]
        p[key] = value
    passengers.append(p)

survived = [p['Survived'] for p in passengers]
pclass = [p['Pclass'] for p in passengers]
age = [float(p['Age']) for p in passengers if p['Age'] != '']
gender_survived = [p['Sex'] for p in passengers if int(p['Survived']) == 1]

import matplotlib.pyplot as plt
from collections import Counter

plt.title("Survived")
plt.pie(Counter(survived).values(), labels=Counter(survived).keys(), autopct='%1.1f%%',
        colors=['lightblue', 'lightgreen'])
plt.show()

plt.title("surviving passengers count by gender")
plt.bar(Counter(gender_survived).keys(), Counter(gender_survived).values())
plt.show()



"""#Chapter 10

"""

import numpy as np
test_scores = [70,65,95,88]
type(test_scores)

scores = np.array(test_scores)
type(scores)

import numpy as np
test_scores = [70,65,95,88]
type(test_scores)

scores = np.array(test_scores)
type(scores)

scores.mean()

import numpy as np
income = np.array([75000, 55000, 88000, 125000, 64000, 97000])

income.mean()

income = np.append(income, 12000000)
income.mean()

np.median(income)

import numpy as np
income = np.array([75000, 55000, 88000, 125000, 64000, 97000])

income.mean()

income = np.append(income, 12000000)

np.median(income)

income.std()

test_scores = [70,65,95,88]

scores = np.array(test_scores)

scores.std()

import numpy as np
np.random.seed(seed=60)
random_square = np.random.rand(5,5)
random_square

# First row
random_square[0]

# First row
random_square[0,:]

# First column
random_square[:,0]

# First entry
random_square[0,0]

# First entry using another way
random_square[0][0]

random_square[2,3]

# Mean entry of matrix
random_square.mean()

# Mean entry of first row
random_square[0].mean()

# Mean entry of last column
random_square[:,-1].mean()

import numpy as np
np.arange(1, 101)

np.arange(1, 101).reshape(20,5)

mat1 = np.arange(1, 101).reshape(20,5)
mat1 - 50

mat1 * 10

mat1 + mat1

mat1*mat1

np.dot(mat1, mat1.T)

import pandas as pd

# Create dictionary of test scores
test_dict = {'Corey':[63,75,88], 'Kevin':[48,98,92], 'Akshay': [87, 86, 85]}

# Create DataFrame
df = pd.DataFrame(test_dict)

# Display DataFrame
df

# Transpose DataFrame
df = df.T
df

import pandas as pd
# Create dictionary of test scores
test_dict = {'Corey':[63,75,88], 'Kevin':[48,98,92], 'Akshay': [87, 86, 85]}
# Create DataFrame
df = pd.DataFrame(test_dict)
df = df.T

# Rename Columns
df.columns = ['Quiz_1', 'Quiz_2', 'Quiz_3']
df

# Access first row by index number
df.iloc[0]

# Access first row by index number
df.iloc[0,:]

# Access first column by name
df['Quiz_1']

# Access first column using dot notation
df.Quiz_1

# Access first column by its index
df.iloc[:, 0]

import pandas as pd
# Create dictionary of test scores
test_dict = {'Corey':[63,75,88], 'Kevin':[48,98,92], 'Akshay': [87, 86, 85]}
# Create DataFrame
df = pd.DataFrame(test_dict)

# Limit DataFrame to first 2 rows
df[0:2]

df = df.T
df

# Rename Columns
df.columns = ['Quiz_1', 'Quiz_2', 'Quiz_3']
df

# Defining a new DataFrame from first 2 rows and last 2 columns
rows = ['Corey', 'Kevin']
cols = ['Quiz_2', 'Quiz_3']
df_spring = df.loc[rows, cols]
df_spring

# Select first 2 rows and last 2 columns using index numbers
df.iloc[[0,1], [1,2]]

# Select first 2 rows and last 2 columns using index numbers
df.iloc[0:2, 1:3]

# Define new column as mean of other columns
df['Quiz_Avg'] = df.mean(axis=1)
df

df['Quiz_4'] = [92, 95, 88]
df

del df['Quiz_Avg']
df

import pandas as pd
# Create dictionary of test scores
test_dict = {'Corey':[63,75,88], 'Kevin':[48,98,92], 'Akshay': [87, 86, 85]}
# Create DataFrame
df = pd.DataFrame(test_dict)

df = df.T
df

# Rename Columns
df.columns = ['Quiz_1', 'Quiz_2', 'Quiz_3']
df

df['Quiz_4'] = [92, 95, 88]
df

import numpy as np
# Create new DataFrame of one row
df_new = pd.DataFrame({'Quiz_1':[np.NaN], 'Quiz_2':[np.NaN], 'Quiz_3': [np.NaN], 'Quiz_4':[71]}, index=['Adrian'])
df = pd.concat([df, df_new])
df

df['Quiz_Avg'] = df.mean(axis=1, skipna=True)
df

import pandas as pd
housing_df = pd.read_csv('HousingData.csv')
housing_df.head()

import pandas as pd
housing_df = pd.read_csv('HousingData.csv')

housing_df.describe()

housing_df.info()

housing_df.shape

import pandas as pd
housing_df = pd.read_csv('HousingData.csv')

housing_df.isnull().any()

housing_df.loc[:5, housing_df.isnull().any()]

housing_df.loc[:, housing_df.isnull().any()].describe()

import pandas as pd
housing_df = pd.read_csv('HousingData.csv')

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

import seaborn as sns
# Set up seaborn dark grid
sns.set()

plt.hist(housing_df['MEDV'])
plt.show()

plt.hist(housing_df['MEDV'])
plt.title('Median Boston Housing Prices')
plt.xlabel('1980 Median Value in Thousands')
plt.ylabel('Count')
plt.show()

title = 'Median Boston Housing Prices'
plt.figure(figsize=(10,6))
plt.hist(housing_df['MEDV'])
plt.title(title, fontsize=15)
plt.xlabel('1980 Median Value in Thousands')
plt.ylabel('Count')
plt.savefig(title, dpi=300)
plt.show()

import pandas as pd
housing_df = pd.read_csv('HousingData.csv')

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

x = housing_df['RM']
y = housing_df['MEDV']
plt.scatter(x, y)
plt.show()

import pandas as pd
housing_df = pd.read_csv('HousingData.csv')

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

housing_df.corr()

import seaborn as sns
# Set up seaborn dark grid
sns.set()

corr = housing_df.corr()
plt.figure(figsize=(8,6))
sns.heatmap(corr, xticklabels=corr.columns.values,
yticklabels=corr.columns.values, cmap="Blues", linewidths=1.25, alpha=0.8)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
housing_df = pd.read_csv('HousingData.csv')
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
# Set up seaborn dark grid
sns.set()

x = housing_df['RM']
y = housing_df['MEDV']
plt.boxplot(x)
plt.show()

#Activity 24

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
# Set up seaborn dark grid
sns.set()

statistics_df = pd.read_csv('UKStatistics.csv')

statistics_df.head()

statistics_df.shape

plt.hist(statistics_df['Actual Pay Floor (£)'])
plt.show()

x = statistics_df['Salary Cost of Reports (£)']
y = statistics_df['Actual Pay Floor (£)']
plt.scatter(x, y)
plt.show()

x = statistics_df['Salary Cost of Reports (£)']
y = statistics_df['Actual Pay Floor (£)']

plt.violinplot(x)
plt.show()

plt.boxplot(x)
plt.show()

