# -*- coding: utf-8 -*-
"""Copy of Homework_5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19CcdBWMkbnM2yEo_7kmxmmSBbZYScf8A

**Homework 5** for the Fall 2024 Course "Machine Learning for Materials Science", University of Tennessee Knoxville, Department of Materials Science and Engineering. Instructor Sergei V. Kalinin

# Graphene data set

We start with the same graphene data set that we explored in class. Run the full analysis in the notebook below.

## Import
"""

# Download the dataset, Will explain what each file corresponds to later
!gdown https://drive.google.com/uc?id=1-JZSRjIjNjkR0ZQ8ffRDAZ2FID53Yhon
!gdown https://drive.google.com/uc?id=1-84vLdGFsimD1jaTcGcMzNRCSvjId7-Y
!gdown https://drive.google.com/uc?id=1-Lowglj7fwEFaJoC9EBKDyfCIsMgOnyu

# Installing AtomAI
# It will be used to split the image into local patches centered on atoms

!pip install atomai

# Importing necessary packages
import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec

# We will use atomai just to create the dataset (sub-images)
import atomai as aoi
import cv2
import torch
import torch.nn as nn

import seaborn as sns
from scipy import stats

tt = torch.tensor

STEM_real = np.load('3DStack13-1-exp.npy')   # raw STEM Image
decoded_imgs = np.load('3DStack13-1-dec.npy')   # oytput of DCNN where the each pixel is classified as one of the three classes (C, Si, or background)
lattice_coord = np.load('3DStack13-1-coord.npy', allow_pickle=True)[()]  # The atomic coodinates found by DCNN

"""## Visualization

For the ease of analysis, let's make the movie shorter
"""

#Vizualizing one frame of the data, we have 50 frames (0-49)

i = 3 # Choose movie frame

# Squeeze the channels in the predicted image (this is optional)
d_img = np.uint8(decoded_imgs[i]*255)
d_img = cv2.cvtColor(d_img, cv2.COLOR_BGR2GRAY)

# Get coordinates for C and Si atoms
lattice_coord_ = lattice_coord[i]
coord_Si = lattice_coord[i][np.where(lattice_coord[i][:,2]==1)][:,0:2]
coord_C = lattice_coord[i][np.where(lattice_coord[i][:,2]==0)][:,0:2]

# Plotting
fig = plt.figure(figsize = (15, 10), dpi = 100)
ax1 = fig.add_subplot(131)
ax1.imshow(STEM_real[i,:,:,0], vmin=0, vmax=0.3, cmap='gray')
ax1.axis('off')
ax1.set_title('Experimental', fontsize=14)
ax2 = fig.add_subplot(132)
ax2.imshow(d_img, cmap='jet', interpolation='Gaussian')
ax2.axis('off')
ax2.set_title('DCNN output', fontsize = 14)
ax3 = fig.add_subplot(133)
ax3.scatter(coord_Si[:,1], coord_Si[:,0], c='red', s=1)
ax3.scatter(coord_C[:,1], coord_C[:,0], c='blue', s=1)
ax3.imshow(STEM_real[i,:,:,0], cmap = 'gray')
ax3.axis('off')
ax3.set_title('Atomic coordinates', fontsize = 14)

"""## Creating features"""

# Getting feature vectors
# Feature vectors here are the cropped images around the coordinates found by DCNN. The window_size of these cropped images can be changed below

window_size = 40   # Window_size

s = aoi.stat.imlocal(
    np.sum(decoded_imgs[..., :-1], -1)[..., None], # convert to a single channel (no background)
    lattice_coord, # Coodinates array, acts as the mid-point of the cropped sub-images
    window_size, 0)

imstack = tt(s.imgstack[:,None,:,:, 0])
frames_all = s.imgstack_frames # will need for plotting VAE results
com_all = s.imgstack_com # will need for plotting VAE results

"""This code has created three objects. imstack is a collectrion of image patches of the size window_size. The object frames_all contain the information on the frame of the movie from which the image patch has come from. The object com_all contains the information on the coordinates of the patch (i.e. position of the atom that is its center) in the frame."""

imstack.shape, frames_all.shape, com_all.shape

np.unique(frames_all)

# Vizualizing the training dataset

np.random.seed(1)  # fix seed so that we get the same samples displayed at every run
fig, axes = plt.subplots(8, 8, figsize=(8, 8),
                         subplot_kw={'xticks':[], 'yticks':[]},
                         gridspec_kw=dict(hspace=0.1, wspace=0.1))

for ax in axes.flat:
    i = np.random.randint(len(imstack))
    ax.imshow(imstack[i, 0], cmap='gnuplot', interpolation='nearest')

# plt.savefig('example_imgs.png', dpi = 300)

"""We can check the sizes of all objects we have created"""

imstack.shape

"""Now, we can select the patches and coordinates corresponding only to the first frame."""

imstack1 = imstack[frames_all == 1]
com_all1 = com_all[frames_all == 1]
imstack1.shape, com_all1.shape

"""## k-means clustering

And run the k-means clustering
"""

from sklearn.cluster import KMeans

a, b, c, d = imstack1.shape
imstack2 = imstack1.reshape(a, c * d)

nc = 3
km = KMeans(n_clusters=nc, random_state=0)
y_km = km.fit_predict(imstack2)

rows = int(np.ceil(float(nc)/5))
cols = int(np.ceil(float(nc)/rows))

gs2 = gridspec.GridSpec(rows, cols)

fig2 = plt.figure(figsize = (4*cols, 4*(1+rows//1.5)))

for i in range(nc):
    ax2 = fig2.add_subplot(gs2[i])
    ax2.imshow(km.cluster_centers_[i,:].reshape(c, d), cmap = 'jet', origin = 'lower')
    ax2.set_title('Component ' + str(i))
    plt.tick_params(labelsize = 18)
    plt.axis('off')
plt.show()

#Visualizing the cluster labels on the raw STEM image

fig, ax = plt.subplots(1, 2, figsize=(30, 15))

ax[0].imshow(STEM_real[i,...,0], cmap="gray")
ax[1].imshow(STEM_real[i,...,0], cmap="gray", alpha = 0.5)
ax[1].scatter(com_all1[:, 1], com_all1[:, 0], c=y_km, s=30, cmap="jet")

plt.show()



"""## GMM clustering"""

from sklearn.mixture import GaussianMixture as GMM

a, b, c, d = imstack1.shape
imstack2 = imstack1.reshape(a, c * d)

nc = 8
gmm = GMM(n_components=nc).fit(imstack2)
gr_labels = gmm.predict(imstack2)
gr_centers = gmm.means_

rows = int(np.ceil(float(nc)/5))
cols = int(np.ceil(float(nc)/rows))

gs2 = gridspec.GridSpec(rows, cols)

fig2 = plt.figure(figsize = (4*cols, 4*(1+rows//1.5)))

for i in range(nc):
    ax2 = fig2.add_subplot(gs2[i])
    ax2.imshow(gr_centers[i,:].reshape(c, d), cmap = 'jet', origin = 'lower')
    ax2.set_title('Component ' + str(i))
    plt.tick_params(labelsize = 18)
    plt.axis('off')
plt.show()

#Visualizing the cluster labels on the raw STEM image

fig, ax = plt.subplots(1, 2, figsize=(30, 15))

ax[0].imshow(STEM_real[i,...,0], cmap="gray")
ax[1].imshow(STEM_real[i,...,0], cmap="gray", alpha = 0.5)
ax[1].scatter(com_all1[:, 1], com_all1[:, 0], c=gr_labels, s=30, cmap="jet")

plt.show()



"""# General comment for homework

Below I provide a set of analyses for Homework 4 using the data above. However, please feel free:
- run this analysis on the EELS data set available in the Notebook 8_Clustering_EELS on https://github.com/SergeiVKalinin/MSE_Fall2023/blob/main/Module%203/8_Clustering_EELS.ipynb
- Or your own dataset as may be relevant to your research. The data set should allow for meaningful classification and clustering
- You can also use different data sets for the two
- use the !gdown method to import the dataset into colab from link (do not use the mounting GDrive option - since I will not be able to access it)

# Homework Part I

1. Calculate the elbow curve for the k-means analysis above
"""

from sklearn.cluster import KMeans

# Reshape imstack1 for clustering
a, b, c, d = imstack1.shape
imstack2 = imstack1.reshape(a, c * d)

# Define the range of clusters to test
cluster_range = range(1, 15)
inertia = []

# Loop through different cluster numbers to calculate inertia
for n_clusters in cluster_range:
    km = KMeans(n_clusters=n_clusters, random_state=0)
    km.fit(imstack2)
    inertia.append(km.inertia_)  # Inertia is the sum of squared distances to the closest cluster center

# Plot the elbow curve
plt.figure(figsize=(8, 6))
plt.plot(cluster_range, inertia, 'bo-', marker='o')
plt.title('Elbow Method for Optimal k')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.grid(True)
plt.show()

"""2. Calculate the silhoutte for the k-means (or GMM - up to you)  for different number of clusters. Determine that optimal number of clusters for this data set and method."""

from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans


a, b, c, d = imstack1.shape
imstack2 = imstack1.reshape(a, c * d)

cluster_range = range(2, 15)
silhouette_scores_kmeans = []

for n_clusters in cluster_range:
    km = KMeans(n_clusters=n_clusters, random_state=0)
    cluster_labels = km.fit_predict(imstack2)
    silhouette_avg = silhouette_score(imstack2, cluster_labels)
    silhouette_scores_kmeans.append(silhouette_avg)

plt.figure(figsize=(8, 6))
plt.plot(cluster_range, silhouette_scores_kmeans, 'bo-', marker='o')
plt.title('Silhouette Score for KMeans')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.show()

# Get the optimal number of clusters
optimal_kmeans_clusters = cluster_range[np.argmax(silhouette_scores_kmeans)]
print(f"Optimal number of clusters for KMeans: {optimal_kmeans_clusters}")

from sklearn.mixture import GaussianMixture


cluster_range = range(2, 15)
silhouette_scores_gmm = []

# silhouette scores
for n_clusters in cluster_range:
    gmm = GaussianMixture(n_components=n_clusters, random_state=0)
    gmm_labels = gmm.fit_predict(imstack2)
    silhouette_avg = silhouette_score(imstack2, gmm_labels)
    silhouette_scores_gmm.append(silhouette_avg)


plt.figure(figsize=(8, 6))
plt.plot(cluster_range, silhouette_scores_gmm, 'bo-', marker='o')
plt.title('Silhouette Score for GMM')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Silhouette Score')
plt.grid(True)
plt.show()

# optimal number of clusters
optimal_gmm_clusters = cluster_range[np.argmax(silhouette_scores_gmm)]
print(f"Optimal number of clusters for GMM: {optimal_gmm_clusters}")

"""3. Repeat this analysis for several wondow sizes (10, 20, 40, 60, 80). What is the optimal number of clusters for each of these window size? Rationalize these observations

Answer 3:
"""

from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score

window_sizes = [10, 20, 40, 60, 80]
cluster_range = range(2, 15)
optimal_clusters_kmeans = {}
optimal_clusters_gmm = {}

for window_size in window_sizes:
    a, b, c, d = imstack1.shape

    if window_size <= c and window_size <= d:
        print(f"Processing window size {window_size}...")

        imstack_resized = imstack1[:, 0, :window_size, :window_size].reshape(a, window_size * window_size)

        silhouette_scores_kmeans = []
        for n_clusters in cluster_range:
            km = KMeans(n_clusters=n_clusters, random_state=0)
            cluster_labels = km.fit_predict(imstack_resized)
            silhouette_avg = silhouette_score(imstack_resized, cluster_labels)
            silhouette_scores_kmeans.append(silhouette_avg)

        optimal_kmeans_clusters = cluster_range[np.argmax(silhouette_scores_kmeans)]
        optimal_clusters_kmeans[window_size] = optimal_kmeans_clusters

        silhouette_scores_gmm = []
        for n_clusters in cluster_range:
            gmm = GaussianMixture(n_components=n_clusters, random_state=0)
            gmm_labels = gmm.fit_predict(imstack_resized)
            silhouette_avg = silhouette_score(imstack_resized, gmm_labels)
            silhouette_scores_gmm.append(silhouette_avg)

        optimal_gmm_clusters = cluster_range[np.argmax(silhouette_scores_gmm)]
        optimal_clusters_gmm[window_size] = optimal_gmm_clusters

        plt.figure(figsize=(10, 6))
        plt.plot(cluster_range, silhouette_scores_kmeans, label=f'KMeans - Window Size {window_size}', marker='o')
        plt.plot(cluster_range, silhouette_scores_gmm, label=f'GMM - Window Size {window_size}', marker='x')
        plt.title(f'Silhouette Scores for Window Size {window_size}')
        plt.xlabel('Number of Clusters')
        plt.ylabel('Silhouette Score')
        plt.legend()
        plt.grid(True)
        plt.show()

    else:
        print(f"Window size {window_size} is too large for the image dimensions ({c}, {d}). Skipping...")


print("Optimal Clusters for KMeans:")
for size, opt_k in optimal_clusters_kmeans.items():
    print(f"Window size {size}: {opt_k} clusters")

print("\nOptimal Clusters for GMM:")
for size, opt_g in optimal_clusters_gmm.items():
    print(f"Window size {size}: {opt_g} clusters")

"""4. In class, we have explored the density based clustering tehcniques. Following the GMM and k-means clustering analysis above, run the same analysis using the density based clustering."""

from sklearn.cluster import DBSCAN
from sklearn.metrics import silhouette_score

window_sizes = [10, 20, 40, 60, 80]
eps_values = np.arange(0.5, 5.5, 0.5)
min_samples = 5

optimal_eps_values = {}

for window_size in window_sizes:
    a, b, c, d = imstack1.shape

    if window_size <= c and window_size <= d:
        print(f"Processing window size {window_size}...")

        imstack_resized = imstack1[:, 0, :window_size, :window_size].reshape(a, window_size * window_size)

        silhouette_scores_dbscan = []

        for eps in eps_values:
            dbscan = DBSCAN(eps=eps, min_samples=min_samples)
            cluster_labels = dbscan.fit_predict(imstack_resized)

            if len(np.unique(cluster_labels)) > 1:
                silhouette_avg = silhouette_score(imstack_resized, cluster_labels)
            else:
                silhouette_avg = -1  # Assign -1 if DBSCAN found no valid clusters

            silhouette_scores_dbscan.append(silhouette_avg)

        optimal_eps = eps_values[np.argmax(silhouette_scores_dbscan)]
        optimal_eps_values[window_size] = optimal_eps

        plt.figure(figsize=(8, 6))
        plt.plot(eps_values, silhouette_scores_dbscan, label=f'DBSCAN - Window Size {window_size}', marker='o')
        plt.title(f'Silhouette Scores for DBSCAN - Window Size {window_size}')
        plt.xlabel('Eps Value')
        plt.ylabel('Silhouette Score')
        plt.grid(True)
        plt.show()

    else:
        print(f"Window size {window_size} is too large for the image dimensions ({c}, {d}). Skipping...")

print("Optimal Eps Values for DBSCAN:")
for size, opt_eps in optimal_eps_values.items():
    print(f"Window size {size}: Optimal eps = {opt_eps}")

"""5. When exploring the analysis for window_size = 40, you will note that symmetry non-equivalent carbon atoms in sp2 configuration are classified into different groups. However, chemically these are equivalent. Suggest potential approach to avoid this problem.

Reassigning atoms that are chemically equivalent but classified into different groups based on their chemical symmetry would be a way to reclassifying atoms and differentiate the cluster centers of symmetry-equivalent atoms.
Another way can be the use of Principal Component Analysis which can be modified to preserve symmetry properties

6 (Optional - if well familiar with Python and want a challenge). Realize this approach
"""

from sklearn.cluster import KMeans, DBSCAN
from sklearn.mixture import GaussianMixture
from scipy.spatial import KDTree

# (sp² configuration)
atomic_positions = lattice_coord[i]

carbon_positions = atomic_positions[atomic_positions[:, 2] == 0][:, :2]

def calculate_coordination(positions, cutoff=1.5):
    """Calculate coordination number for each atom based on nearest neighbors."""
    kdtree = KDTree(positions)
    coord_numbers = np.array([len(kdtree.query_ball_point(pos, cutoff)) - 1 for pos in positions]) # Exclude self
    return coord_numbers

coord_numbers = calculate_coordination(carbon_positions)
carbon_features = np.column_stack((carbon_positions, coord_numbers))

kmeans = KMeans(n_clusters=3, random_state=0)
clusters = kmeans.fit_predict(carbon_features)

plt.scatter(carbon_positions[:, 0], carbon_positions[:, 1], c=clusters, cmap='jet')
plt.title('Initial Clustering Result')
plt.show()

def group_symmetric_atoms(positions, clusters, coord_numbers, tol=0.1):
    """Merge clusters for symmetrically equivalent atoms based on spatial and structural similarity."""
    new_clusters = clusters.copy()

    for i in range(len(positions)):
        for j in range(i + 1, len(positions)):
            if np.linalg.norm(positions[i] - positions[j]) < tol and np.abs(coord_numbers[i] - coord_numbers[j]) < tol:
                new_clusters[j] = new_clusters[i]

    return new_clusters

new_clusters = group_symmetric_atoms(carbon_positions, clusters, coord_numbers)

plt.scatter(carbon_positions[:, 0], carbon_positions[:, 1], c=new_clusters, cmap='jet')
plt.title('Clustering with Symmetry Constraints')
plt.show()

"""7. (Optional - if well familiar with Python and want a challenge). Analyze the stack of images using k-means clustering and plot the evolution of cluster fractions as a function of time.

# Homework Part II

After running the analysis in the previous sections, we now have access to the k-means clustering results of the image patches. Effectively, each atom in the image is now associated with the patch centered on this atom, and the cluster label. The y_km is a label array, and km.cluster_centers_ is the centroid of the clustes. Remember that you need to reshape it back to (window_size, window_size) in order to get the image. We also have access to the imstack2 object, which is a collection of all patches, and com_all1 object that contains the information on the coordinate from which the patch has come from.
"""

y_km.shape, km.cluster_centers_.shape, imstack2.shape, com_all1.shape

"""8. Create a X,y data set for a classification problem. Here, the image is a feature, and the cluster label is assumed to be known ground truth label (alternatively, we can do manual labelling, but for the purpose of excercise let's assume that ground truth labels are already known)."""

import torch

imstack1 = torch.randn(1798, 1, 40, 40)  # Replace this with the actual loading method

# Proceed with the rest of your code
imstack2 = imstack1.reshape(1798, 40 * 40)  # Flatten the image patches

print(f"imstack1 shape: {imstack1.shape}")
print(f"imstack2 shape: {imstack2.shape}")

y = y_km
print("X shape:", X.shape)
print("y shape:", y.shape)

# Check the shape of imstack2
print("imstack2 shape:", imstack2.shape)

X = imstack2
y = y_km
print("Feature shape (X):", X.shape)
print("Label shape (y):", y.shape)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
classifier = RandomForestClassifier(n_estimators=100, random_state=42)


classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)

from sklearn.model_selection import train_test_split

X = imstack2
y = y_km

# Step 1: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 2: Verify the shapes of the resulting splits
print("Training feature shape:", X_train.shape)
print("Training label shape:", y_train.shape)
print("Testing feature shape:", X_test.shape)
print("Testing label shape:", y_test.shape)

clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)


y_pred = clf.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

"""9. Use the train_test_split function to create the training set and test set for classification problem."""

from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

X = imstack2
y = y_km

# Step 2: Split the dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

# Predict on the test set
y_pred = clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print("Test set accuracy:", accuracy)

feature_importances = clf.feature_importances_
plt.figure(figsize=(10, 6))
plt.bar(range(len(feature_importances)), feature_importances)
plt.xlabel('Feature Index')
plt.ylabel('Importance')
plt.title('Importance OF Feature in Random Forest')
plt.show()

"""10. Run the logistics classification on the data and plot the confucion matrix and ROC curve"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import seaborn as sns


X = imstack2
y = y_km

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Regression classifier
logreg = LogisticRegression(max_iter=1000, random_state=42)
logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)
y_pred_proba = logreg.predict_proba(X_test)

conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()


fpr = {}
tpr = {}
roc_auc = {}
n_classes = len(np.unique(y))

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test, y_pred_proba[:, i], pos_label=i)
    roc_auc[i] = auc(fpr[i], tpr[i])

plt.figure(figsize=(10, 8))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label=f'Class {i} (area = {roc_auc[i]:0.2f})')

plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

print(classification_report(y_test, y_pred))

"""11. Run the CART classification on the data and plot the decision tree and ROC curve. Is this analysis meaningful?

Answer 11:
"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree


X = imstack2
y = y_km

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

cart = DecisionTreeClassifier(random_state=42)
cart.fit(X_train, y_train)

y_pred = cart.predict(X_test)
y_pred_proba = cart.predict_proba(X_test)

conf_matrix = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

fpr = {}
tpr = {}
roc_auc = {}
n_classes = len(np.unique(y))

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test, y_pred_proba[:, i], pos_label=i)
    roc_auc[i] = auc(fpr[i], tpr[i])

plt.figure(figsize=(10, 8))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label=f'Class {i} (area = {roc_auc[i]:0.2f})')

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

#Plot the decision tree
plt.figure(figsize=(20, 10))
plot_tree(cart, filled=True, feature_names=[f'pixel_{i}' for i in range(X_train.shape[1])], class_names=[str(i) for i in np.unique(y)], rounded=True)
plt.title("Decision Tree (CART) Visualization")
plt.show()

print(classification_report(y_test, y_pred))

"""# Overfitting is resulted in the data.

12. Run the XGBoost classification on the dataset and plot the ROC curve.
"""

import xgboost as xgb
from sklearn.model_selection import train_test_split


X = imstack2
y = y_km

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
xgb_model.fit(X_train, y_train)


y_pred = xgb_model.predict(X_test)
y_pred_proba = xgb_model.predict_proba(X_test)
conf_matrix = confusion_matrix(y_test, y_pred)

# confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

fpr = {}
tpr = {}
roc_auc = {}
n_classes = len(np.unique(y))

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test, y_pred_proba[:, i], pos_label=i)
    roc_auc[i] = auc(fpr[i], tpr[i])

# ROC curves
plt.figure(figsize=(10, 8))
for i in range(n_classes):
    plt.plot(fpr[i], tpr[i], label=f'Class {i} (area = {roc_auc[i]:0.2f})')

plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

print(classification_report(y_test, y_pred))

