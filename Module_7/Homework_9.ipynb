{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swapnamoy007/ML-ASSIGNMENT/blob/main/Module_7/Homework_9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Homework 8 for the Fall 2024 Course \"Machine Learning for Materials Science\", University of Tennessee Knoxville, Department of Materials Science and Engineering.\n",
        "\n",
        "- Instructor Sergei V. Kalinin"
      ],
      "metadata": {
        "id": "xzAAwIsgVugY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please add your name and date"
      ],
      "metadata": {
        "id": "n52vu8MWVy-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Swapnamoy dutta ; 11/12/2024"
      ],
      "metadata": {
        "id": "fvsrKoan-KkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer 0:"
      ],
      "metadata": {
        "id": "moUxTuGWV1tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this homework, we will explore the use of ChatGPT as a code assistant to help understand and operationalize neural networks. As we discussed during the class, ChatGPT does an excellent work with simple code development - as long as the tasks are well formulated and we can check the code and results."
      ],
      "metadata": {
        "id": "_79OveR_V4Dh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fashion MNIST classifier"
      ],
      "metadata": {
        "id": "oVFJVFq5XS9Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example prompt: \"Write a Python code importing fashion MNIST data set and visualizing 10 random images along with the labels\""
      ],
      "metadata": {
        "id": "AE8GRNbzXeya"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "BX7T6bFFVmHn",
        "outputId": "267c116a-b13e-43fd-da95-0cf8fa57c2df"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# Select 10 random indices\n",
        "random_indices = np.random.choice(len(train_images), 10)\n",
        "\n",
        "# Plot images and labels\n",
        "for i, idx in enumerate(random_indices):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[idx], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[idx]])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Ask ChatGPT to write a DCNN classifier for Fashion-MNIST data. Specify that the network should use the dropout layers and normalization layers. Code should save the trained network weights, and also save history (loss and accuracy)."
      ],
      "metadata": {
        "id": "rjuTPjNvXWxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Load the Fashion MNIST data\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "# Reshape data for the convolutional layers\n",
        "train_images = train_images.reshape(-1, 28, 28, 1)\n",
        "test_images = test_images.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Define the class names\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Define the model architecture\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Flatten(),\n",
        "\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Define a checkpoint callback to save the model weights with the .keras extension\n",
        "checkpoint_path = \"fashion_mnist_dcnn_weights.keras\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_path, save_best_only=True, monitor='val_accuracy', mode='max'\n",
        ")\n",
        "\n",
        "# Train the model and save training history\n",
        "history = model.fit(train_images, train_labels, epochs=15,\n",
        "                    validation_data=(test_images, test_labels),\n",
        "                    callbacks=[checkpoint])\n",
        "\n",
        "# Save training history (loss and accuracy)\n",
        "history_data = {\n",
        "    'loss': history.history['loss'],\n",
        "    'val_loss': history.history['val_loss'],\n",
        "    'accuracy': history.history['accuracy'],\n",
        "    'val_accuracy': history.history['val_accuracy']\n",
        "}\n",
        "\n",
        "# Save the history data\n",
        "with open(\"fashion_mnist_training_history.pkl\", \"wb\") as file:\n",
        "    pickle.dump(history_data, file)\n",
        "\n",
        "# Plot the training and validation accuracy and loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ncFtwooSXYl4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ea88c5-5836-4a87-a4a4-2e016ed48be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m1051/1875\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7041 - loss: 0.8818"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and normalize Fashion MNIST data\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "train_images = train_images.reshape(-1, 28, 28, 1)\n",
        "test_images = test_images.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Define the DCNN model with dropout and batch normalization\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Flatten(),\n",
        "\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile and train the model with checkpointing for best model weights\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "checkpoint_path = \"fashion_mnist_dcnn_weights.keras\"\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "history = model.fit(train_images, train_labels, epochs=15, validation_data=(test_images, test_labels), callbacks=[checkpoint])\n",
        "\n",
        "# Save training history\n",
        "with open(\"fashion_mnist_training_history.pkl\", \"wb\") as file:\n",
        "    pickle.dump(history.history, file)\n",
        "\n",
        "# Plot the training/validation accuracy and loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rx_FyUSXAaav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Plot the evolution of the loss and accuracy for trianing and test data"
      ],
      "metadata": {
        "id": "L_1ozZE-YcZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "\n",
        "# Load training history\n",
        "with open(\"fashion_mnist_training_history.pkl\", \"rb\") as file:\n",
        "    history_data = pickle.load(file)\n",
        "\n",
        "# Plot the training and validation accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_data['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history_data['val_accuracy'], label='Test Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_data['loss'], label='Training Loss')\n",
        "plt.plot(history_data['val_loss'], label='Test Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XTv22HyJYk5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Ask ChatGPT to write the code to plot the ROC curve for the classifier"
      ],
      "metadata": {
        "id": "SPQSvZmxYlFa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess the test data\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(_, _), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "test_images = test_images / 255.0  # Normalize\n",
        "test_images = test_images.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Binarize the test labels for multi-class ROC\n",
        "test_labels_binarized = label_binarize(test_labels, classes=range(10))\n",
        "\n",
        "# Load the trained model (optional, if it needs to be reloaded)\n",
        "model = tf.keras.models.load_model(\"fashion_mnist_dcnn_weights.keras\")\n",
        "\n",
        "# Get model predictions (probabilities)\n",
        "y_score = model.predict(test_images)\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i in range(10):\n",
        "    fpr, tpr, _ = roc_curve(test_labels_binarized[:, i], y_score[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "# Plot formatting\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Fashion-MNIST Classifier')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7kAuV_7qYs95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Ask ChatGPT (and provide answers below):\n",
        "- What is the dimensionality of the input to the classifier?\n",
        "- What is the dimensionality of the output?\n",
        "- Why the loss function is chosen?\n",
        "- What is the metrics for performance?\n",
        "- What is the role of MaxPool layers?\n",
        "- What is the role of batch normalization layers?\n",
        "- Can we use the Relu rather then softmax in the final layer\n",
        "- What is the function of dropout layers?"
      ],
      "metadata": {
        "id": "1xbpYRJHYtIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality of the Input to the Classifier:\n",
        "\n",
        "The input dimensionality to the classifier is (28, 28, 1). This represents 28x28 pixel grayscale images, where the 1 channel indicates that these images are single-channel (grayscale) as opposed to three-channel (RGB).\n",
        "Dimensionality of the Output:\n",
        "\n",
        "The output dimensionality is 10, corresponding to the 10 possible classes in the Fashion-MNIST dataset. Each output node represents the probability of the input image belonging to one of the classes (e.g., T-shirt/top, Trouser, Pullover, etc.).\n",
        "Why the Loss Function is Chosen:\n",
        "\n",
        "The loss function used is sparse categorical cross-entropy, which is appropriate for multi-class classification tasks where each image belongs to one of the distinct classes. Cross-entropy loss measures the difference between the true labels and the predicted probability distribution, penalizing incorrect predictions more severely. Sparse categorical cross-entropy is used here instead of categorical cross-entropy because the labels are in integer form rather than one-hot encoded.\n",
        "Metric for Performance:\n",
        "\n",
        "Accuracy is used as the main metric for performance, as it provides a straightforward measure of how many predictions are correct out of the total predictions. Accuracy is commonly used in balanced datasets where each class has similar importance, which suits Fashion-MNIST well.\n",
        "Role of MaxPooling Layers:\n",
        "\n",
        "MaxPooling layers reduce the spatial dimensions (height and width) of the input representation by selecting the maximum value within each patch of the feature map. This operation helps to:\n",
        "Decrease the computational load and memory requirements.\n",
        "Make the model more invariant to small translations in the input (shift invariance).\n",
        "Retain the most salient features, as only the maximum values are kept.\n",
        "Role of Batch Normalization Layers:\n",
        "\n",
        "Batch normalization layers normalize the inputs of each layer to have a mean of 0 and a standard deviation of 1. This helps in:\n",
        "Reducing the internal covariate shift, which stabilizes and accelerates training.\n",
        "Allowing for higher learning rates, potentially leading to faster convergence.\n",
        "Acting as a form of regularization, which can help in reducing overfitting.\n",
        "Can We Use ReLU Instead of Softmax in the Final Layer?:\n",
        "\n",
        "No, we cannot use ReLU (Rectified Linear Activation Unit) instead of Softmax in the final layer for a classification task.\n",
        "Softmax converts the output into a probability distribution across classes, ensuring that all class probabilities sum to 1.\n",
        "ReLU is a linear activation that outputs raw scores, which cannot be interpreted as probabilities. Therefore, Softmax is required in the final layer for multi-class classification to provide interpretable class probabilities.\n",
        "Function of Dropout Layers:\n",
        "\n",
        "Dropout layers randomly deactivate a fraction of neurons in the layer during training. This helps in:\n",
        "Reducing overfitting by preventing the model from becoming too reliant on specific neurons.\n",
        "Encouraging the network to learn more robust features, as it must learn to make predictions even with certain neurons turned off.\n",
        "Acting as a form of regularization by introducing noise into the training process, which improves generalization on unseen data."
      ],
      "metadata": {
        "id": "k1gpG4HfBGU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answers:\n"
      ],
      "metadata": {
        "id": "o6ae9JZ6Zavx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Perform the ablation study. Rewrite the code so that:\n",
        "- we change the number of neurons (let's say divide it by factor of two several times)\n",
        "- save the training curve\n",
        "- save the ROC curve\n",
        "And then plot them jointly. For example, if originally you had neural network with (128, 256) neurons, then you will generate training and ROC curves for (128,256), (64,  128), (32, 64), AND (16, 32)."
      ],
      "metadata": {
        "id": "_7Ut55KsZft6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import os\n",
        "\n",
        "# Load and preprocess Fashion MNIST data\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "train_images = train_images.reshape(-1, 28, 28, 1)\n",
        "test_images = test_images.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Binarize the test labels for ROC\n",
        "test_labels_binarized = label_binarize(test_labels, classes=range(10))\n",
        "\n",
        "# Initial neuron configuration for the dense layers\n",
        "neuron_configs = [(128, 256), (64, 128), (32, 64), (16, 32)]\n",
        "history_dict = {}\n",
        "roc_data = {}\n",
        "\n",
        "# Directory for saving results\n",
        "if not os.path.exists(\"ablation_study_results\"):\n",
        "    os.makedirs(\"ablation_study_results\")\n",
        "\n",
        "for config in neuron_configs:\n",
        "    print(f\"Training model with dense layer configuration: {config}\")\n",
        "\n",
        "    # Define the model\n",
        "    model = models.Sequential([\n",
        "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Dropout(0.25),\n",
        "\n",
        "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Flatten(),\n",
        "\n",
        "        layers.Dense(config[0], activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(config[1], activation='relu'),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model and save history\n",
        "    history = model.fit(\n",
        "        train_images, train_labels,\n",
        "        epochs=10, validation_data=(test_images, test_labels),\n",
        "        verbose=2\n",
        "    )\n",
        "\n",
        "    # Save training history\n",
        "    history_dict[config] = history.history\n",
        "\n",
        "    # Generate predictions and ROC data\n",
        "    y_score = model.predict(test_images)\n",
        "    fpr_dict, tpr_dict, auc_dict = {}, {}, {}\n",
        "    for i in range(10):\n",
        "        fpr, tpr, _ = roc_curve(test_labels_binarized[:, i], y_score[:, i])\n",
        "        fpr_dict[i] = fpr\n",
        "        tpr_dict[i] = tpr\n",
        "        auc_dict[i] = auc(fpr, tpr)\n",
        "\n",
        "    roc_data[config] = {'fpr': fpr_dict, 'tpr': tpr_dict, 'auc': auc_dict}\n",
        "\n",
        "# Save all data for later use\n",
        "with open(\"ablation_study_results/training_history.pkl\", \"wb\") as file:\n",
        "    pickle.dump(history_dict, file)\n",
        "\n",
        "with open(\"ablation_study_results/roc_data.pkl\", \"wb\") as file:\n",
        "    pickle.dump(roc_data, file)\n",
        "\n",
        "# Plotting training curves for all configurations\n",
        "plt.figure(figsize=(15, 6))\n",
        "for config in neuron_configs:\n",
        "    # Training accuracy and validation accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history_dict[config]['accuracy'], label=f'Train Acc - {config}')\n",
        "    plt.plot(history_dict[config]['val_accuracy'], '--', label=f'Val Acc - {config}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training & Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Training loss and validation loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history_dict[config]['loss'], label=f'Train Loss - {config}')\n",
        "    plt.plot(history_dict[config]['val_loss'], '--', label=f'Val Loss - {config}')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training & Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot ROC curves for all configurations\n",
        "for config in neuron_configs:\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    for i in range(10):\n",
        "        plt.plot(roc_data[config]['fpr'][i], roc_data[config]['tpr'][i],\n",
        "                 label=f'Class {i} (AUC = {roc_data[config][\"auc\"][i]:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(f'ROC Curve for Dense Layer Config: {config}')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "VXsRl0QFZfKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fashion MNIST VAE"
      ],
      "metadata": {
        "id": "YqegYOqeabwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Ask Chat GPT to write the simple MLP AE for Fashion MNIST data. Visualzie the latent distribution and latent representations."
      ],
      "metadata": {
        "id": "NokH6ExZZba6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load and preprocess Fashion MNIST data\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "# Flatten the images for MLP input\n",
        "train_images_flat = train_images.reshape(-1, 28 * 28)\n",
        "test_images_flat = test_images.reshape(-1, 28 * 28)\n",
        "\n",
        "# Define MLP Autoencoder architecture\n",
        "encoding_dim = 64  # Dimension of the latent space\n",
        "\n",
        "# Encoder\n",
        "input_img = layers.Input(shape=(784,))\n",
        "encoded = layers.Dense(128, activation='relu')(input_img)\n",
        "encoded = layers.Dense(64, activation='relu')(encoded)\n",
        "latent = layers.Dense(encoding_dim, activation='relu')(encoded)\n",
        "\n",
        "# Decoder\n",
        "decoded = layers.Dense(64, activation='relu')(latent)\n",
        "decoded = layers.Dense(128, activation='relu')(decoded)\n",
        "output_img = layers.Dense(784, activation='sigmoid')(decoded)\n",
        "\n",
        "# Build and compile the autoencoder model\n",
        "autoencoder = models.Model(input_img, output_img)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# Train the autoencoder\n",
        "history = autoencoder.fit(train_images_flat, train_images_flat,\n",
        "                          epochs=20,\n",
        "                          batch_size=256,\n",
        "                          shuffle=True,\n",
        "                          validation_data=(test_images_flat, test_images_flat))\n",
        "\n",
        "# Encoder model for extracting latent representations\n",
        "encoder = models.Model(input_img, latent)\n",
        "latent_representations = encoder.predict(test_images_flat)\n",
        "\n",
        "# Visualize the training loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Autoencoder Training Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Visualize the latent space (2D or 3D projection for simplicity)\n",
        "# Use PCA or t-SNE to reduce the dimensionality of the latent space if needed\n",
        "\n",
        "# Visualize the latent representations with t-SNE\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "latent_2d = tsne.fit_transform(latent_representations)\n",
        "\n",
        "# Plot the latent space with labels\n",
        "plt.figure(figsize=(10, 8))\n",
        "scatter = plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=test_labels, cmap='tab10', alpha=0.6)\n",
        "plt.colorbar(scatter, ticks=range(10), label='Classes')\n",
        "plt.xlabel('Latent Dimension 1')\n",
        "plt.ylabel('Latent Dimension 2')\n",
        "plt.title('2D Visualization of Latent Representations (t-SNE)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ugmf3jnlarU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Ask Chat GPT to write the DCNN VAE AE for Fashion MNIST data. Visualzie the latent distribution and latent representations. Modify the program to save the latent represnetation and latent distributions every 5 steps as images."
      ],
      "metadata": {
        "id": "F5pYdK9EZXGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import os\n",
        "\n",
        "# Load and preprocess Fashion MNIST data\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "train_images = train_images.reshape(-1, 28, 28, 1)\n",
        "test_images = test_images.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Latent dimension\n",
        "latent_dim = 2  # Set to 2 for easier visualization\n",
        "\n",
        "# Encoder\n",
        "input_img = layers.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(input_img)\n",
        "x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation='relu')(x)\n",
        "\n",
        "# Latent space\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "\n",
        "# Sampling layer\n",
        "class Sampling(layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "\n",
        "# Decoder\n",
        "decoder_input = layers.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(7 * 7 * 64, activation='relu')(decoder_input)\n",
        "x = layers.Reshape((7, 7, 64))(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n",
        "decoded_img = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "# Define encoder and decoder models\n",
        "encoder = Model(input_img, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "decoder = Model(decoder_input, decoded_img, name=\"decoder\")\n",
        "\n",
        "# VAE Model\n",
        "output = decoder(encoder(input_img)[2])\n",
        "vae = Model(input_img, output, name=\"vae\")\n",
        "\n",
        "# Custom VAE Loss Layer\n",
        "class VAELossLayer(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(VAELossLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_img, output, z_mean, z_log_var = inputs\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "            tf.keras.losses.binary_crossentropy(input_img, output)\n",
        "        ) * 28 * 28\n",
        "        kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "        self.add_loss(reconstruction_loss + kl_loss)\n",
        "        return output\n",
        "\n",
        "# Apply the custom loss layer\n",
        "vae_output = VAELossLayer()([input_img, output, z_mean, z_log_var])\n",
        "vae = Model(input_img, vae_output, name=\"vae_with_loss\")\n",
        "\n",
        "# Compile the model\n",
        "vae.compile(optimizer='adam')\n",
        "\n",
        "# Directory for saving images\n",
        "if not os.path.exists(\"latent_visualizations\"):\n",
        "    os.makedirs(\"latent_visualizations\")\n",
        "\n",
        "# Training callback to save latent representations and distributions\n",
        "class LatentVisualizationCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, encoder, interval=5):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.interval = interval\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.interval == 0:\n",
        "            # Get the latent representations\n",
        "            z_mean, _, z = self.encoder.predict(test_images)\n",
        "\n",
        "            # t-SNE for 2D projection if latent_dim > 2\n",
        "            if latent_dim > 2:\n",
        "                z_2d = TSNE(n_components=2).fit_transform(z)\n",
        "            else:\n",
        "                z_2d = z\n",
        "\n",
        "            # Plot latent space distribution\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.scatter(z_2d[:, 0], z_2d[:, 1], c=test_labels, cmap='tab10', alpha=0.7)\n",
        "            plt.colorbar()\n",
        "            plt.xlabel(\"Latent Dimension 1\")\n",
        "            plt.ylabel(\"Latent Dimension 2\")\n",
        "            plt.title(f\"Latent Space Distribution at Epoch {epoch+1}\")\n",
        "            plt.savefig(f\"latent_visualizations/latent_distribution_epoch_{epoch+1}.png\")\n",
        "            plt.close()\n",
        "\n",
        "# Train the VAE model and save latent distributions every 5 epochs\n",
        "vae.fit(train_images, train_images,\n",
        "        epochs=50,\n",
        "        batch_size=128,\n",
        "        validation_data=(test_images, test_images),\n",
        "        callbacks=[LatentVisualizationCallback(encoder)])\n",
        "\n",
        "# Plot final latent space distribution after training\n",
        "z_mean, _, z = encoder.predict(test_images)\n",
        "z_2d = TSNE(n_components=2).fit_transform(z) if latent_dim > 2 else z\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(z_2d[:, 0], z_2d[:, 1], c=test_labels, cmap='tab10', alpha=0.7)\n",
        "plt.colorbar()\n",
        "plt.xlabel(\"Latent Dimension 1\")\n",
        "plt.ylabel(\"Latent Dimension 2\")\n",
        "plt.title(\"Final Latent Space Distribution\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BUSOmRNted-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Can you upload the images and make a movie?"
      ],
      "metadata": {
        "id": "tmlDcbtUefLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "# Directory where images are saved\n",
        "image_dir = \"latent_visualizations\"\n",
        "output_video_path = \"latent_distributions_video.mp4\"\n",
        "\n",
        "# Get all image paths and sort by filename to ensure correct order\n",
        "image_paths = sorted(glob(os.path.join(image_dir, \"*.png\")))\n",
        "\n",
        "# Check if we have images to process\n",
        "if not image_paths:\n",
        "    print(\"No images found in the specified directory. Ensure images were saved correctly.\")\n",
        "else:\n",
        "    # Read the first image to get dimensions\n",
        "    frame = cv2.imread(image_paths[0])\n",
        "    height, width, layers = frame.shape\n",
        "\n",
        "    # Initialize video writer\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Video codec for mp4\n",
        "    video = cv2.VideoWriter(output_video_path, fourcc, 1, (width, height))  # 1 frame per second\n",
        "\n",
        "    # Add each image as a frame in the video\n",
        "    for image_path in image_paths:\n",
        "        frame = cv2.imread(image_path)\n",
        "        video.write(frame)\n",
        "\n",
        "    # Release the video writer\n",
        "    video.release()\n",
        "    print(f\"Video saved as {output_video_path}\")\n"
      ],
      "metadata": {
        "id": "1KvYYbIzbBVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write the defintions for:\n",
        "- Latent istribution\n",
        "- Latent represnetation\n",
        "- Disentangling of represnetations\n",
        "- Conditional VAE"
      ],
      "metadata": {
        "id": "aJxZk7j1bBpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answers:\n",
        "Latent Distribution\n",
        "A latent distribution is a probability distribution over the latent variables in a model, representing the underlying factors that describe the essential features or variations in the observed data. In the context of VAEs, the latent distribution is typically modeled as a Gaussian distribution with parameters (mean and variance) derived from the encoder network. This distribution defines the possible values for the latent variables, which are sampled during training and decoding to generate or reconstruct data instances.\n",
        "\n",
        "Latent Representation\n",
        "A latent representation is a compressed, encoded form of the data that captures its essential characteristics in a low-dimensional space. This representation is learned by the encoder in a VAE and serves as a hidden, often more abstract, version of the original data. Latent representations are essential in machine learning because they allow models to focus on the underlying structure of the data rather than superficial details, which is useful for tasks such as generation, clustering, and classification.\n",
        "\n",
        "Disentangling of Representations\n",
        "Disentangling of representations refers to the process of separating different generative factors within a latent representation, so that each latent variable or dimension represents a distinct aspect of the data. In a disentangled representation, changes to one latent variable would ideally influence only one aspect of the data, allowing for more interpretable and controllable representations. Disentangled representations are valuable for applications like image generation, where controlling individual latent factors (e.g., lighting, orientation) can produce varied but specific outputs.\n",
        "\n",
        "Conditional Variational Autoencoder (Conditional VAE)\n",
        "A Conditional VAE is an extension of the Variational Autoencoder that incorporates additional conditioning information (such as class labels or other attributes) into the encoding and decoding process. By conditioning the model on this extra information, Conditional VAEs can learn to generate samples that align with specified conditions, effectively allowing for more targeted control over the generated data. For example, in an image generation task, a Conditional VAE could generate images of a particular category (e.g., dogs or cats) based on a label input during both training and sampling."
      ],
      "metadata": {
        "id": "AtDZOoy5dM7T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Explore if the ChatGPT can write a code for the VAE with rotational invariance"
      ],
      "metadata": {
        "id": "-abrKvyWdNU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Define encoder\n",
        "def build_encoder(latent_dim):\n",
        "    inputs = layers.Input(shape=(28, 28, 1))\n",
        "    x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(inputs)\n",
        "    x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "\n",
        "    z_mean = layers.Dense(latent_dim)(x)\n",
        "    z_log_var = layers.Dense(latent_dim)(x)\n",
        "\n",
        "    encoder = models.Model(inputs, [z_mean, z_log_var], name=\"encoder\")\n",
        "    encoder.summary()\n",
        "    return encoder\n",
        "\n",
        "# Sampling function to sample from the latent space\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    z = z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "    return z\n",
        "\n",
        "# Define decoder\n",
        "def build_decoder(latent_dim):\n",
        "    latent_inputs = layers.Input(shape=(latent_dim,))\n",
        "    x = layers.Dense(256, activation='relu')(latent_inputs)\n",
        "    x = layers.Dense(7 * 7 * 64, activation='relu')(x)\n",
        "    x = layers.Reshape((7, 7, 64))(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "    x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n",
        "    outputs = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    decoder = models.Model(latent_inputs, outputs, name=\"decoder\")\n",
        "    decoder.summary()\n",
        "    return decoder\n",
        "\n",
        "# VAE loss function\n",
        "def vae_loss(inputs, output_img, z_mean, z_log_var):\n",
        "    # Flatten both inputs and output for binary cross-entropy\n",
        "    flattened_inputs = K.flatten(inputs)\n",
        "    flattened_output_img = K.flatten(output_img)\n",
        "\n",
        "    # Compute the reconstruction loss (binary cross-entropy)\n",
        "    reconstruction_loss = tf.reduce_mean(\n",
        "        tf.keras.losses.binary_crossentropy(flattened_inputs, flattened_output_img)\n",
        "    )\n",
        "    reconstruction_loss *= 28 * 28  # Scale the reconstruction loss\n",
        "\n",
        "    # Compute the KL divergence loss\n",
        "    kl_loss = -0.5 * tf.reduce_sum(\n",
        "        1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1\n",
        "    )\n",
        "\n",
        "    # Return the total VAE loss (sum of reconstruction and KL losses)\n",
        "    return tf.reduce_mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "# Build VAE model\n",
        "latent_dim = 2  # Latent space dimension\n",
        "encoder = build_encoder(latent_dim)\n",
        "decoder = build_decoder(latent_dim)\n",
        "\n",
        "# Encoder output\n",
        "z_mean, z_log_var = encoder.output\n",
        "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "# Decoder output\n",
        "output_img = decoder(z)\n",
        "\n",
        "# Define VAE model\n",
        "vae = models.Model(encoder.input, output_img, name=\"vae\")\n",
        "\n",
        "# Add the VAE loss to the model\n",
        "vae.add_loss(lambda: vae_loss(encoder.input, output_img, z_mean, z_log_var))\n",
        "vae.compile(optimizer=\"adam\")\n",
        "\n",
        "# Print the model summary\n",
        "vae.summary()\n",
        "\n",
        "# Train the VAE\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255.\n",
        "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255.\n",
        "\n",
        "# Data generator for training\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "\n",
        "vae.fit(x_train, x_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=(x_test, x_test))\n"
      ],
      "metadata": {
        "id": "jO3mNpRSN2A5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Define encoder\n",
        "def build_encoder(latent_dim):\n",
        "    inputs = layers.Input(shape=(28, 28, 1))\n",
        "    x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(inputs)\n",
        "    x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "\n",
        "    z_mean = layers.Dense(latent_dim)(x)\n",
        "    z_log_var = layers.Dense(latent_dim)(x)\n",
        "\n",
        "    encoder = models.Model(inputs, [z_mean, z_log_var], name=\"encoder\")\n",
        "    encoder.summary()\n",
        "    return encoder\n",
        "\n",
        "# Sampling function to sample from the latent space\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    z = z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "    return z\n",
        "\n",
        "# Define decoder\n",
        "def build_decoder(latent_dim):\n",
        "    latent_inputs = layers.Input(shape=(latent_dim,))\n",
        "    x = layers.Dense(256, activation='relu')(latent_inputs)\n",
        "    x = layers.Dense(7 * 7 * 64, activation='relu')(x)\n",
        "    x = layers.Reshape((7, 7, 64))(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "    x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n",
        "    outputs = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    decoder = models.Model(latent_inputs, outputs, name=\"decoder\")\n",
        "    decoder.summary()\n",
        "    return decoder\n",
        "\n",
        "# VAE loss function\n",
        "def vae_loss(inputs, output_img, z_mean, z_log_var):\n",
        "    # Flatten both inputs and output for binary cross-entropy\n",
        "    flattened_inputs = K.flatten(inputs)\n",
        "    flattened_output_img = K.flatten(output_img)\n",
        "\n",
        "    # Compute the reconstruction loss (binary cross-entropy)\n",
        "    reconstruction_loss = tf.reduce_mean(\n",
        "        tf.keras.losses.binary_crossentropy(flattened_inputs, flattened_output_img)\n",
        "    )\n",
        "    reconstruction_loss *= 28 * 28  # Scale the reconstruction loss\n",
        "\n",
        "    # Compute the KL divergence loss\n",
        "    kl_loss = -0.5 * tf.reduce_sum(\n",
        "        1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1\n",
        "    )\n",
        "\n",
        "    # Return the total VAE loss (sum of reconstruction and KL losses)\n",
        "    return tf.reduce_mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "# Build VAE model\n",
        "latent_dim = 2  # Latent space dimension\n",
        "encoder = build_encoder(latent_dim)\n",
        "decoder = build_decoder(latent_dim)\n",
        "\n",
        "# Encoder output\n",
        "z_mean, z_log_var = encoder.output\n",
        "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "# Decoder output\n",
        "output_img = decoder(z)\n",
        "\n",
        "# Define VAE model\n",
        "vae = models.Model(encoder.input, output_img, name=\"vae\")\n",
        "\n",
        "# Compile the model\n",
        "vae.compile(optimizer=\"adam\")\n",
        "\n",
        "# Print the model summary\n",
        "vae.summary()\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255.\n",
        "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255.\n",
        "\n",
        "# Data generator for training\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "\n",
        "# Custom training loop\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        x_batch_train = x_train[i:i + batch_size]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean_batch, z_log_var_batch = encoder(x_batch_train)\n",
        "            z_batch = sampling([z_mean_batch, z_log_var_batch])\n",
        "            output_batch = decoder(z_batch)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = vae_loss(x_batch_train, output_batch, z_mean_batch, z_log_var_batch)\n",
        "\n",
        "        # Compute gradients and update model weights\n",
        "        grads = tape.gradient(loss, vae.trainable_variables)\n",
        "        vae.optimizer.apply_gradients(zip(grads, vae.trainable_variables))\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.numpy()}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "output_test = vae.predict(x_test)\n"
      ],
      "metadata": {
        "id": "k3a_midzQ5kP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Bu_o_Df5N1In"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, backend as K\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Define encoder with data augmentation for rotational invariance\n",
        "def build_encoder(latent_dim):\n",
        "    inputs = layers.Input(shape=(28, 28, 1))\n",
        "    x = layers.RandomRotation(0.2)(inputs)  # Apply random rotations to enforce rotational invariance\n",
        "    x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(x)\n",
        "    x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "\n",
        "    z_mean = layers.Dense(latent_dim)(x)\n",
        "    z_log_var = layers.Dense(latent_dim)(x)\n",
        "\n",
        "    encoder = models.Model(inputs, [z_mean, z_log_var], name=\"encoder\")\n",
        "    encoder.summary()\n",
        "    return encoder\n",
        "\n",
        "# Sampling function for reparameterization trick\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# Define decoder\n",
        "def build_decoder(latent_dim):\n",
        "    latent_inputs = layers.Input(shape=(latent_dim,))\n",
        "    x = layers.Dense(256, activation='relu')(latent_inputs)\n",
        "    x = layers.Dense(7 * 7 * 64, activation='relu')(x)\n",
        "    x = layers.Reshape((7, 7, 64))(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "    x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n",
        "    outputs = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    decoder = models.Model(latent_inputs, outputs, name=\"decoder\")\n",
        "    decoder.summary()\n",
        "    return decoder\n",
        "\n",
        "# VAE loss function\n",
        "def vae_loss(inputs, outputs, z_mean, z_log_var):\n",
        "    # Flatten both inputs and output for binary cross-entropy\n",
        "    flattened_inputs = K.flatten(inputs)\n",
        "    flattened_outputs = K.flatten(outputs)\n",
        "\n",
        "    # Reconstruction loss (binary cross-entropy)\n",
        "    reconstruction_loss = tf.reduce_mean(\n",
        "        tf.keras.losses.binary_crossentropy(flattened_inputs, flattened_outputs)\n",
        "    )\n",
        "    reconstruction_loss *= 28 * 28  # Scale by image size\n",
        "\n",
        "    # KL divergence loss\n",
        "    kl_loss = -0.5 * tf.reduce_sum(\n",
        "        1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1\n",
        "    )\n",
        "    return tf.reduce_mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "# Build VAE\n",
        "latent_dim = 2\n",
        "encoder = build_encoder(latent_dim)\n",
        "decoder = build_decoder(latent_dim)\n",
        "\n",
        "# Define inputs, sampling, and outputs\n",
        "z_mean, z_log_var = encoder.output\n",
        "z = layers.Lambda(sampling, name=\"sampling\")([z_mean, z_log_var])\n",
        "outputs = decoder(z)\n",
        "\n",
        "# Define the VAE model\n",
        "vae = models.Model(encoder.input, outputs, name=\"vae\")\n",
        "\n",
        "# Compile VAE model with a custom training loop\n",
        "vae.compile(optimizer=\"adam\")\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255.\n",
        "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255.\n",
        "\n",
        "# Set up training parameters\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "\n",
        "# Custom training loop\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "    for i in range(0, len(x_train), batch_size):\n",
        "        x_batch_train = x_train[i:i + batch_size]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass\n",
        "            z_mean_batch, z_log_var_batch = encoder(x_batch_train)\n",
        "            z_batch = sampling([z_mean_batch, z_log_var_batch])\n",
        "            output_batch = decoder(z_batch)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = vae_loss(x_batch_train, output_batch, z_mean_batch, z_log_var_batch)\n",
        "\n",
        "        # Compute gradients and update model weights\n",
        "        grads = tape.gradient(loss, vae.trainable_variables)\n",
        "        vae.optimizer.apply_gradients(zip(grads, vae.trainable_variables))\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss.numpy()}\")\n",
        "\n",
        "# Evaluate the VAE on the test set\n",
        "output_test = vae.predict(x_test)\n"
      ],
      "metadata": {
        "id": "8UXmDuXvbW32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answers:"
      ],
      "metadata": {
        "id": "HBSo2BkydXfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Ask ChatGPT to write a class-conditioned VAE and plot the latent represnetations."
      ],
      "metadata": {
        "id": "WFpsvMh-dZLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda, Flatten, Reshape, Concatenate, Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "train_images = np.expand_dims(train_images, -1) / 255.0\n",
        "test_images = np.expand_dims(test_images, -1) / 255.0\n",
        "\n",
        "# Define image dimensions, latent space size, and number of classes\n",
        "img_shape = (28, 28, 1)\n",
        "latent_dim = 2\n",
        "num_classes = 10\n",
        "\n",
        "# Convert labels to one-hot vectors for conditioning\n",
        "train_labels_onehot = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
        "test_labels_onehot = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
        "\n",
        "# Define the encoder model with conditioning\n",
        "inputs = Input(shape=img_shape)\n",
        "label_input = Input(shape=(num_classes,))  # Condition input\n",
        "\n",
        "# Encoder network\n",
        "x = Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(inputs)\n",
        "x = Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = Flatten()(x)\n",
        "x = Concatenate()([x, label_input])  # Concatenate label conditioning\n",
        "x = Dense(16, activation=\"relu\")(x)\n",
        "z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "\n",
        "# Sampling function\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# Latent vector\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name=\"z\")([z_mean, z_log_var])\n",
        "z_cond = Concatenate()([z, label_input])  # Concatenate label conditioning to latent space\n",
        "\n",
        "# Define the decoder model\n",
        "decoder_inputs = Input(shape=(latent_dim + num_classes,))\n",
        "x = Dense(7 * 7 * 64, activation=\"relu\")(decoder_inputs)\n",
        "x = Reshape((7, 7, 64))(x)\n",
        "x = Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "outputs = Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "\n",
        "# Build encoder and decoder models\n",
        "encoder = Model([inputs, label_input], [z_mean, z_log_var, z], name=\"encoder\")\n",
        "decoder = Model(decoder_inputs, outputs, name=\"decoder\")\n",
        "output_img = decoder(z_cond)\n",
        "cvae = Model([inputs, label_input], output_img, name=\"cvae\")\n",
        "\n",
        "# Define VAE loss function using a custom layer\n",
        "class VAELossLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        inputs, outputs, z_mean, z_log_var = inputs\n",
        "        # Compute reconstruction loss\n",
        "        reconstruction_loss = binary_crossentropy(tf.keras.layers.Flatten()(inputs), tf.keras.layers.Flatten()(outputs))\n",
        "        reconstruction_loss *= 28 * 28\n",
        "        # Compute KL divergence loss\n",
        "        kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "        kl_loss = -0.5 * K.sum(kl_loss, axis=-1)\n",
        "        # Total loss\n",
        "        return K.mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "# Apply the loss layer\n",
        "loss_layer = VAELossLayer()([inputs, output_img, z_mean, z_log_var])\n",
        "cvae.add_loss(loss_layer)\n",
        "cvae.compile(optimizer=\"adam\")\n",
        "\n",
        "# Train the model\n",
        "batch_size = 128\n",
        "epochs = 50\n",
        "cvae.fit([train_images, train_labels_onehot], train_images,\n",
        "         epochs=epochs,\n",
        "         batch_size=batch_size,\n",
        "         validation_data=([test_images, test_labels_onehot], test_images))\n",
        "\n",
        "# Plot latent space for each class\n",
        "def plot_latent_space(encoder, data, labels, num_classes=10):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for i in range(num_classes):\n",
        "        # Select data points of the current class\n",
        "        indices = np.where(labels == i)[0]\n",
        "        class_data = data[indices]\n",
        "        class_labels = tf.keras.utils.to_categorical(labels[indices], num_classes)\n",
        "\n",
        "        # Predict mean of the latent space\n",
        "        z_mean, _, _ = encoder.predict([class_data, class_labels])\n",
        "\n",
        "        # Plot latent space with different colors for each class\n",
        "        plt.scatter(z_mean[:, 0], z_mean[:, 1], label=str(i), alpha=0.6)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.legend(title=\"Class\")\n",
        "    plt.title(\"Latent Space of Conditioned VAE by Class\")\n",
        "    plt.show()\n",
        "\n",
        "# Visualize latent space for test images\n",
        "plot_latent_space(encoder, test_images, test_labels)\n"
      ],
      "metadata": {
        "id": "GKRp7eEidgpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda, Flatten, Reshape, Concatenate, Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Load and preprocess Fashion MNIST data\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "train_images = np.expand_dims(train_images, -1) / 255.0\n",
        "test_images = np.expand_dims(test_images, -1) / 255.0\n",
        "num_classes = 10\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "# Convert labels to one-hot for conditioning\n",
        "train_labels_onehot = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
        "test_labels_onehot = tf.keras.utils.to_categorical(test_labels, num_classes)\n",
        "\n",
        "# Model parameters\n",
        "img_shape = (28, 28, 1)\n",
        "latent_dim = 2\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=img_shape)\n",
        "label_input = Input(shape=(num_classes,))\n",
        "x = Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(inputs)\n",
        "x = Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = Flatten()(x)\n",
        "x = Concatenate()([x, label_input])\n",
        "x = Dense(16, activation=\"relu\")(x)\n",
        "z_mean = Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "\n",
        "# Sampling function\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], latent_dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name=\"z\")([z_mean, z_log_var])\n",
        "z_cond = Concatenate()([z, label_input])\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(latent_dim + num_classes,))\n",
        "x = Dense(7 * 7 * 64, activation=\"relu\")(decoder_inputs)\n",
        "x = Reshape((7, 7, 64))(x)\n",
        "x = Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "outputs = Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "\n",
        "# Models\n",
        "encoder = Model([inputs, label_input], [z_mean, z_log_var, z], name=\"encoder\")\n",
        "decoder = Model(decoder_inputs, outputs, name=\"decoder\")\n",
        "cvae_outputs = decoder(z_cond)\n",
        "cvae = Model([inputs, label_input], cvae_outputs, name=\"cvae\")\n",
        "\n",
        "# VAE loss\n",
        "reconstruction_loss = tf.reduce_sum(\n",
        "    binary_crossentropy(K.flatten(inputs), K.flatten(cvae_outputs)), axis=-1\n",
        ")\n",
        "reconstruction_loss *= 28 * 28\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = -0.5 * K.sum(kl_loss, axis=-1)\n",
        "cvae.add_loss(K.mean(reconstruction_loss + kl_loss))\n",
        "cvae.compile(optimizer=\"adam\")\n",
        "\n",
        "# Train\n",
        "cvae.fit([train_images, train_labels_onehot], train_images,\n",
        "         epochs=50,\n",
        "         batch_size=128,\n",
        "         validation_data=([test_images, test_labels_onehot], test_images))\n",
        "\n",
        "# Plot latent space for each class\n",
        "def plot_latent_space(encoder, data, labels, num_classes=10):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for i in range(num_classes):\n",
        "        indices = np.where(labels == i)[0]\n",
        "        class_data = data[indices]\n",
        "        class_labels = tf.keras.utils.to_categorical(labels[indices], num_classes)\n",
        "        z_mean, _, _ = encoder.predict([class_data, class_labels])\n",
        "        plt.scatter(z_mean[:, 0], z_mean[:, 1], label=class_names[i], alpha=0.6)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.legend(title=\"Class\")\n",
        "    plt.title(\"Latent Space of Conditioned VAE by Class\")\n",
        "    plt.show()\n",
        "\n",
        "plot_latent_space(encoder, test_images, test_labels)\n"
      ],
      "metadata": {
        "id": "cSdg4M24tcvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Can you identify the meaning of the latent variables?"
      ],
      "metadata": {
        "id": "nX717raCeHHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answers:\n",
        "In a Variational Autoencoder (VAE), the latent variables represent compressed representations of the original data. However, they typically lack direct interpretability, especially if the VAE is trained without any explicit supervision or additional constraints to shape their meaning. The latent space variables capture underlying features of the data, such as general characteristics, but their exact meaning depends on the data structure and how the VAE organizes that information.\n",
        "\n",
        "Here’s a breakdown of how we might interpret or understand the meaning of latent variables in a VAE:\n",
        "\n",
        "1. Feature Extraction and Encoding Complex Patterns\n",
        "VAEs learn a compressed representation of the data, typically by clustering similar data points in the latent space. The dimensions of this space capture variations and correlations in the dataset. For example, with image data like MNIST digits, latent variables might organize information about the general shape, orientation, or stroke style.\n",
        "Since our latent space here is 2D, we might see a smooth transition between digit shapes or intensity when we visualize the latent space. For instance, as you move across the latent space, the generated images might shift gradually from one digit to another, revealing how the VAE learned to represent different classes within the continuous latent space.\n",
        "2. Understanding Latent Variables through Interpolation and Sampling\n",
        "By sampling points along a line between two locations in latent space, we can observe how the VAE “morphs” one image into another. This provides insight into the \"directions\" the model considers important in the latent space. If sampling between two latent points results in gradual changes in brightness, orientation, or shape, then the VAE might be encoding those characteristics along that direction.\n",
        "3. Cluster Visualization in Latent Space\n",
        "When you reduce the latent space to two dimensions and visualize it (as in our code above), natural clustering can reveal the presence of meaningful groupings. For example, in the MNIST dataset, the VAE’s latent space might loosely group similar digits together (e.g., all '0's near each other, '1's together), even though no labels were provided. This shows the VAE has learned features that correspond to digit similarity.\n",
        "4. Latent Variables for Anomaly Detection\n",
        "In anomaly detection, the latent variables can indicate how \"normal\" or \"anomalous\" an image is relative to the training data distribution. Images that reconstruct poorly (i.e., have a high reconstruction error) suggest the VAE has encoded information that doesn’t match well with typical samples in the dataset. Anomalies in the latent space often have latent representations that lie outside of the high-density areas (typical samples) in the space.\n",
        "5. Adding Supervision or Constraints to Interpret Latent Variables\n",
        "Without additional constraints, latent variables tend to capture general, abstract patterns rather than specific, interpretable ones. For example, adding a \"conditional\" element (like conditioning on class labels) to a VAE could force the latent variables to represent specific features related to each class. This approach is called a Conditional VAE (CVAE) and can make latent variables more interpretable by design."
      ],
      "metadata": {
        "id": "nuTy2idueLA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load MNIST dataset\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255\n",
        "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# VAE model parameters\n",
        "latent_dim = 2  # For 2D latent space\n",
        "\n",
        "# Encoder model\n",
        "encoder_inputs = tf.keras.Input(shape=input_shape)\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "\n",
        "# Sampling layer\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(tf.keras.backend.shape(z_mean)[0], latent_dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = layers.Lambda(sampling, output_shape=(latent_dim,), name=\"z\")([z_mean, z_log_var])\n",
        "\n",
        "# Encoder model\n",
        "encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "encoder.summary()\n",
        "\n",
        "# Decoder model\n",
        "decoder_inputs = tf.keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(decoder_inputs)\n",
        "x = layers.Reshape((7, 7, 64))(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "\n",
        "# Decoder model\n",
        "decoder = Model(decoder_inputs, decoder_outputs, name=\"decoder\")\n",
        "decoder.summary()\n",
        "\n",
        "# VAE model\n",
        "outputs = decoder(encoder(encoder_inputs)[2])\n",
        "vae = Model(encoder_inputs, outputs, name=\"vae\")\n",
        "\n",
        "# VAE Loss function\n",
        "reconstruction_loss = tf.keras.losses.binary_crossentropy(encoder_inputs, outputs)\n",
        "reconstruction_loss *= 28 * 28\n",
        "kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "kl_loss = tf.reduce_sum(kl_loss, axis=-1) * -0.5\n",
        "vae_loss = tf.reduce_mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer=\"adam\")\n",
        "\n",
        "# Train the VAE\n",
        "vae.fit(x_train, x_train, epochs=50, batch_size=128, validation_data=(x_test, x_test))\n",
        "\n",
        "# Visualization of the 2D latent space\n",
        "n = 20  # Grid size\n",
        "grid_x = np.linspace(-3, 3, n)\n",
        "grid_y = np.linspace(-3, 3, n)\n",
        "\n",
        "figure = np.zeros((28 * n, 28 * n))\n",
        "for i, yi in enumerate(grid_y):\n",
        "    for j, xi in enumerate(grid_x):\n",
        "        z_sample = np.array([[xi, yi]])\n",
        "        x_decoded = decoder.predict(z_sample)\n",
        "        digit = x_decoded[0].reshape(28, 28)\n",
        "        figure[i * 28: (i + 1) * 28,\n",
        "               j * 28: (j + 1) * 28] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap=\"gray\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TyvcB5yE12fE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Explore with ChatGPT how VAEs are used for (one of):\n",
        "- Image analysis\n",
        "- Deep fakes\n",
        "- Natural Language Programming\n",
        "- Drug discovery\n",
        "- Property optimization"
      ],
      "metadata": {
        "id": "Cmo6-Sh2eqV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, backend as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Define the encoder\n",
        "def build_encoder(latent_dim):\n",
        "    inputs = layers.Input(shape=(28, 28, 1))\n",
        "    x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(inputs)\n",
        "    x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "\n",
        "    z_mean = layers.Dense(latent_dim)(x)\n",
        "    z_log_var = layers.Dense(latent_dim)(x)\n",
        "\n",
        "    encoder = models.Model(inputs, [z_mean, z_log_var], name=\"encoder\")\n",
        "    return encoder\n",
        "\n",
        "# Sampling function for reparameterization\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "# Define the decoder\n",
        "def build_decoder(latent_dim):\n",
        "    latent_inputs = layers.Input(shape=(latent_dim,))\n",
        "    x = layers.Dense(256, activation='relu')(latent_inputs)\n",
        "    x = layers.Dense(7 * 7 * 64, activation='relu')(x)\n",
        "    x = layers.Reshape((7, 7, 64))(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "    x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n",
        "    outputs = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n",
        "\n",
        "    decoder = models.Model(latent_inputs, outputs, name=\"decoder\")\n",
        "    return decoder\n",
        "\n",
        "# VAE loss\n",
        "def vae_loss(inputs, outputs, z_mean, z_log_var):\n",
        "    reconstruction_loss = tf.reduce_mean(\n",
        "        tf.keras.losses.binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n",
        "    ) * 28 * 28\n",
        "    kl_loss = -0.5 * tf.reduce_sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
        "    return tf.reduce_mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "# Hyperparameters\n",
        "latent_dim = 2\n",
        "\n",
        "# Build encoder and decoder\n",
        "encoder = build_encoder(latent_dim)\n",
        "decoder = build_decoder(latent_dim)\n",
        "\n",
        "# Connect encoder and decoder with the sampling layer\n",
        "z_mean, z_log_var = encoder.output\n",
        "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
        "outputs = decoder(z)\n",
        "\n",
        "# Define the VAE model\n",
        "vae = models.Model(encoder.input, outputs, name=\"vae\")\n",
        "\n",
        "# Compile the VAE with custom loss\n",
        "vae.add_loss(vae_loss(encoder.input, outputs, z_mean, z_log_var))\n",
        "vae.compile(optimizer=\"adam\")\n",
        "\n",
        "# Load and preprocess MNIST data\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = np.expand_dims(x_train, -1).astype(\"float32\") / 255.\n",
        "x_test = np.expand_dims(x_test, -1).astype(\"float32\") / 255.\n",
        "\n",
        "# Train the VAE\n",
        "vae.fit(x_train, epochs=10, batch_size=128, validation_data=(x_test, None))\n",
        "\n",
        "# 1. Image Generation\n",
        "n = 10  # Number of images to generate\n",
        "fig, ax = plt.subplots(1, n, figsize=(20, 4))\n",
        "for i in range(n):\n",
        "    z_sample = np.random.normal(size=(1, latent_dim))\n",
        "    generated_img = decoder.predict(z_sample)\n",
        "    ax[i].imshow(generated_img[0, :, :, 0], cmap='gray')\n",
        "    ax[i].axis('off')\n",
        "plt.suptitle(\"Generated Images\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Denoising\n",
        "# Add noise to testing data\n",
        "noise_factor = 0.5\n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
        "\n",
        "# Denoise images by encoding and decoding\n",
        "denoised_imgs = vae.predict(x_test_noisy)\n",
        "\n",
        "# Display original, noisy, and denoised images\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 6))\n",
        "for i in range(n):\n",
        "    # Original\n",
        "    ax = plt.subplot(3, n, i + 1)\n",
        "    plt.imshow(x_test[i].reshape(28, 28), cmap=\"gray\")\n",
        "    ax.axis(\"off\")\n",
        "    # Noisy\n",
        "    ax = plt.subplot(3, n, i + 1 + n)\n",
        "    plt.imshow(x_test_noisy[i].reshape(28, 28), cmap=\"gray\")\n",
        "    ax.axis(\"off\")\n",
        "    # Denoised\n",
        "    ax = plt.subplot(3, n, i + 1 + 2 * n)\n",
        "    plt.imshow(denoised_imgs[i].reshape(28, 28), cmap=\"gray\")\n",
        "    ax.axis(\"off\")\n",
        "plt.suptitle(\"Denoising Images with VAE\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Anomaly Detection\n",
        "# Reconstruction errors for normal test images\n",
        "reconstruction_errors = []\n",
        "for img in x_test:\n",
        "    img = np.expand_dims(img, 0)\n",
        "    reconstructed_img = vae.predict(img)\n",
        "    reconstruction_error = vae_loss(img, reconstructed_img, *encoder.predict(img))\n",
        "    reconstruction_errors.append(reconstruction_error.numpy())\n",
        "\n",
        "# Set a threshold for anomaly detection\n",
        "threshold = np.percentile(reconstruction_errors, 95)\n",
        "\n",
        "# Detect anomalies\n",
        "anomalies = [x_test[i] for i, error in enumerate(reconstruction_errors) if error > threshold]\n",
        "\n",
        "# Display some anomalies\n",
        "n = min(len(anomalies), 10)\n",
        "plt.figure(figsize=(20, 2))\n",
        "for i in range(n):\n",
        "    ax = plt.subplot(1, n, i + 1)\n",
        "    plt.imshow(anomalies[i].reshape(28, 28), cmap='gray')\n",
        "    ax.axis(\"off\")\n",
        "plt.suptitle(\"Anomalies Detected by VAE\")\n",
        "plt.show()\n",
        "\n",
        "# 4. Dimensionality Reduction for Visualization\n",
        "# Project test images into the 2D latent space\n",
        "z_mean, _ = encoder.predict(x_test)\n",
        "\n",
        "# Plot the 2D latent space\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(z_mean[:, 0], z_mean[:, 1], c='b', alpha=0.5, s=2)\n",
        "plt.xlabel(\"z[0]\")\n",
        "plt.ylabel(\"z[1]\")\n",
        "plt.title(\"2D Latent Space of Test Images\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Yb-6bFVNcV9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rdkit-pypi\n"
      ],
      "metadata": {
        "id": "NdLhLM8bwoVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda, Embedding, LSTM\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "\n",
        "# Example: Load or create your SMILES dataset\n",
        "train_smiles = [\n",
        "    'CCO', 'CCN', 'CCC', 'COC', 'CN', 'CCCO', 'CCCC', 'CC(=O)O', 'CC(=O)N', 'CC(C)O'\n",
        "]  # This should be replaced with your actual dataset\n",
        "\n",
        "# Define characters used in SMILES strings (including special symbols and characters)\n",
        "smiles_chars = 'CNO=()[]#$%0123456789abcdefghijklmnopqrstuvwxyz'\n",
        "char_to_idx = {char: idx for idx, char in enumerate(smiles_chars)}\n",
        "idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
        "\n",
        "# Encode SMILES strings as integers (one-hot encoding)\n",
        "def encode_smiles(smiles, char_to_idx):\n",
        "    return [char_to_idx.get(char, 0) for char in smiles]  # 0 is used for unknown characters\n",
        "\n",
        "# Encode all SMILES strings in the dataset\n",
        "encoded_smiles = [encode_smiles(smiles, char_to_idx) for smiles in train_smiles]\n",
        "\n",
        "# Set max sequence length for padding\n",
        "max_sequence_length = 20  # Adjust based on your data\n",
        "\n",
        "# Pad sequences to ensure consistent input size\n",
        "train_data = pad_sequences(encoded_smiles, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Expand dimensions to add a channel for single input dimensions (e.g., one-hot encoding)\n",
        "train_data_expanded = np.expand_dims(train_data, -1)\n",
        "\n",
        "# Define the VAE model\n",
        "latent_dim = 2  # Set the latent dimensionality for the VAE\n",
        "\n",
        "# Encoder\n",
        "inputs = Input(shape=(max_sequence_length,))\n",
        "embedding_layer = Embedding(input_dim=len(smiles_chars), output_dim=32)(inputs)\n",
        "x = LSTM(64)(embedding_layer)\n",
        "z_mean = Dense(latent_dim)(x)\n",
        "z_log_var = Dense(latent_dim)(x)\n",
        "\n",
        "# Sampling function for the VAE\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
        "\n",
        "# Decoder\n",
        "latent_inputs = Input(shape=(latent_dim,))\n",
        "x = Dense(64, activation='relu')(latent_inputs)\n",
        "x = Dense(max_sequence_length, activation='softmax')(x)\n",
        "\n",
        "decoder = Model(latent_inputs, x)\n",
        "\n",
        "# Define the VAE loss function\n",
        "def vae_loss(inputs, vae_outputs, z_mean, z_log_var):\n",
        "    reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(vae_outputs)) * max_sequence_length\n",
        "    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "    kl_loss = -0.5 * K.sum(kl_loss, axis=-1)\n",
        "    return K.mean(reconstruction_loss + kl_loss)\n",
        "\n",
        "# Define the VAE model that includes the decoder and computes the loss during the forward pass\n",
        "vae_outputs = decoder(z)\n",
        "vae = Model(inputs, vae_outputs)\n",
        "\n",
        "# Custom training loop\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "@tf.function\n",
        "def train_step(data):\n",
        "    with tf.GradientTape() as tape:\n",
        "        inputs, labels = data\n",
        "        vae_outputs = vae(inputs)  # Forward pass\n",
        "        loss = vae_loss(inputs, vae_outputs, z_mean, z_log_var)  # Compute the loss\n",
        "    gradients = tape.gradient(loss, vae.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, vae.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "# Prepare dataset for training\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_data, train_data))\n",
        "train_dataset = train_dataset.batch(32)\n",
        "\n",
        "# Training loop\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for data in train_dataset:\n",
        "        loss = train_step(data)\n",
        "        total_loss += loss\n",
        "    print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss.numpy()}')\n",
        "\n",
        "# Generate new SMILES using the decoder\n",
        "def decode_latent(latent_variables):\n",
        "    decoded = decoder.predict(latent_variables)\n",
        "    decoded_indices = np.argmax(decoded, axis=-1)\n",
        "    decoded_smiles = [''.join([idx_to_char[i] for i in smile]) for smile in decoded_indices]\n",
        "    return decoded_smiles\n",
        "\n",
        "# Sampling from the latent space and decoding\n",
        "latent_samples = np.random.normal(size=(10, latent_dim))\n",
        "generated_smiles = decode_latent(latent_samples)\n",
        "print(generated_smiles)\n"
      ],
      "metadata": {
        "id": "42ASCJgbwXpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example"
      ],
      "metadata": {
        "id": "ft_JiybzePDC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tuhEhuf-wWET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Example: VAE written by ChatGPT"
      ],
      "metadata": {
        "id": "IYM93RHrdg2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Reshape, Conv2DTranspose, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "image_size = x_train.shape[1]\n",
        "x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n",
        "x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n",
        "\n",
        "latent_dim = 2  # Dimensionality of the latent space\n",
        "\n",
        "inputs = Input(shape=(image_size, image_size, 1), name='encoder_input')\n",
        "x = Conv2D(32, 3, activation='relu', strides=2, padding='same')(inputs)\n",
        "x = Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "x = Flatten()(x)\n",
        "x = Dense(16, activation='relu')(x)\n",
        "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
        "\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    batch = K.shape(z_mean)[0]\n",
        "    dim = K.int_shape(z_mean)[1]\n",
        "    epsilon = K.random_normal(shape=(batch, dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "\n",
        "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "x = Dense(7 * 7 * 64, activation='relu')(latent_inputs)\n",
        "x = Reshape((7, 7, 64))(x)\n",
        "x = Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n",
        "x = Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n",
        "outputs = Conv2DTranspose(1, 3, activation='sigmoid', padding='same', name='decoder_output')(x)\n",
        "\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae')\n",
        "\n",
        "reconstruction_loss = binary_crossentropy(K.flatten(inputs), K.flatten(outputs))\n",
        "reconstruction_loss *= image_size * image_size\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "vae.compile(optimizer='adam')\n",
        "\n",
        "vae.fit(x_train, epochs=30, batch_size=128, validation_data=(x_test, None))\n",
        "\n",
        "def plot_label_clusters(encoder, data, labels):\n",
        "    z_mean, _, _ = encoder.predict(data)\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=labels)\n",
        "    plt.colorbar()\n",
        "    plt.xlabel(\"z[0]\")\n",
        "    plt.ylabel(\"z[1]\")\n",
        "    plt.show()\n",
        "\n",
        "plot_label_clusters(encoder, x_test, y_test)\n",
        "\n",
        "n = 15\n",
        "digit_size = 28\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "\n",
        "grid_x = np.linspace(-4, 4, n)\n",
        "grid_y = np.linspace(-4, 4, n)\n",
        "\n",
        "for i, yi in enumerate(grid_x):\n",
        "    for j, xi in enumerate(grid_y):\n",
        "        z_sample = np.array([[xi, yi]])\n",
        "        x_decoded = decoder.predict(z_sample)\n",
        "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "        figure[i * digit_size: (i + 1) * digit_size,\n",
        "               j * digit_size: (j + 1) * digit_size] = digit\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(figure, cmap='Greys_r')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ckzqi6bWcV5U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}